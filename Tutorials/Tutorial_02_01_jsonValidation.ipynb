{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to Pydantic for LLM Workflows\n",
    "\n",
    "This notebook provides a hands-on guide to using Pydantic for structuring and validating LLM outputs. We'll explore core concepts, practical examples, and best practices for building robust LLM-powered applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages and set up our AWS Bedrock connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pydantic boto3 langchain-aws email-validator python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime, date\n",
    "from typing import Optional, List, Literal\n",
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field, HttpUrl, ValidationError, validator\n",
    "from langchain_aws import ChatBedrock\n",
    "from google.colab import userdata\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS credentials using Colab secrets\n",
    "AWS_ACCESS_KEY_ID = userdata.get('awsid')\n",
    "AWS_SECRET_ACCESS_KEY = userdata.get('awssecret')\n",
    "AWS_REGION = \"us-east-1\"\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "\n",
    "# Set up the Bedrock model (using Amazon Nova Lite for cost-effectiveness)\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.nova-lite-v1:0\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ AWS Bedrock client initialized\")\n",
    "print(f\"✓ Using model: amazon.nova-lite-v1:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction and Context\n",
    "\n",
    "### The Challenge of Structured Output from LLMs\n",
    "\n",
    "When working with Large Language Models (LLMs), one of the fundamental challenges is obtaining structured, predictable output that can be reliably processed by downstream systems. While you can simply ask an LLM to format its response in a particular way (like JSON), the results are often unpredictable:\n",
    "\n",
    "**Common Issues:**\n",
    "- Extra text outside the JSON structure (e.g., \"Here's the JSON output you requested:\")\n",
    "- Markdown formatting (triple backticks around JSON)\n",
    "- Missing or incorrectly formatted fields\n",
    "- Invalid data types\n",
    "\n",
    "### Why Pydantic?\n",
    "\n",
    "Pydantic provides a robust solution by allowing you to:\n",
    "1. Define explicit data models with field names and types\n",
    "2. Validate LLM responses against these models\n",
    "3. Catch and handle validation errors systematically\n",
    "4. Ensure data consistency throughout your application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Pydantic Models\n",
    "\n",
    "Let's start by creating simple Pydantic models and understanding how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserInput(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    query: str\n",
    "\n",
    "# Valid data\n",
    "user_input = UserInput(\n",
    "    name=\"Alice Johnson\",\n",
    "    email=\"alice.johnson@company.com\",\n",
    "    query=\"I need help resetting my account password\"\n",
    ")\n",
    "\n",
    "print(\"Valid user input:\")\n",
    "print(user_input)\n",
    "print(f\"\\nName: {user_input.name}\")\n",
    "print(f\"Email: {user_input.email}\")\n",
    "print(f\"Query: {user_input.query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invalid email - missing @ symbol\n",
    "try:\n",
    "    invalid_user = UserInput(\n",
    "        name=\"Bob Smith\",\n",
    "        email=\"bob.smith.invalid.com\",  # Invalid format\n",
    "        query=\"Where is my order?\"\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(\"Validation Error Occurred:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-commerce Support System Example\n",
    "\n",
    "Let's build a more realistic model for a customer support ticketing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IssueType(str, Enum):\n",
    "    DELIVERY = \"delivery\"\n",
    "    PRODUCT_QUALITY = \"product_quality\"\n",
    "    BILLING = \"billing\"\n",
    "    TECHNICAL = \"technical\"\n",
    "    OTHER = \"other\"\n",
    "\n",
    "class SupportTicket(BaseModel):\n",
    "    customer_name: str = Field(min_length=2, max_length=100)\n",
    "    email: EmailStr\n",
    "    phone: Optional[str] = Field(default=None, pattern=r\"^\\+?1?\\d{9,15}$\")\n",
    "    issue_type: IssueType\n",
    "    description: str = Field(min_length=10, max_length=1000)\n",
    "    order_number: Optional[str] = Field(\n",
    "        default=None,\n",
    "        pattern=r\"^ORD-\\d{8}$\",\n",
    "        description=\"Format: ORD-12345678\"\n",
    "    )\n",
    "    purchase_date: Optional[date] = None\n",
    "    attachments: List[str] = Field(default_factory=list, max_items=5)\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"customer_name\": \"Sarah Chen\",\n",
    "                \"email\": \"sarah.chen@email.com\",\n",
    "                \"phone\": \"+1-555-0123\",\n",
    "                \"issue_type\": \"product_quality\",\n",
    "                \"description\": \"The laptop I received has a defective screen with dead pixels\",\n",
    "                \"order_number\": \"ORD-20240315\",\n",
    "                \"purchase_date\": \"2024-03-15\",\n",
    "                \"attachments\": [\"screen_issue.jpg\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create a support ticket\n",
    "ticket = SupportTicket(\n",
    "    customer_name=\"Sarah Chen\",\n",
    "    email=\"sarah.chen@email.com\",\n",
    "    issue_type=IssueType.PRODUCT_QUALITY,\n",
    "    description=\"The laptop I received has a defective screen with dead pixels\",\n",
    "    order_number=\"ORD-20240315\"\n",
    ")\n",
    "\n",
    "print(\"Support Ticket Created:\")\n",
    "print(json.dumps(ticket.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Coercion\n",
    "\n",
    "Pydantic automatically converts compatible data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product(BaseModel):\n",
    "    product_id: int\n",
    "    price: float\n",
    "    in_stock: bool\n",
    "    \n",
    "# String to int/float/bool conversion\n",
    "product = Product(\n",
    "    product_id=\"12345\",      # Converted to int\n",
    "    price=\"29.99\",           # Converted to float\n",
    "    in_stock=\"true\"          # Converted to bool\n",
    ")\n",
    "\n",
    "print(f\"Product ID: {product.product_id} (type: {type(product.product_id).__name__})\")\n",
    "print(f\"Price: {product.price} (type: {type(product.price).__name__})\")\n",
    "print(f\"In Stock: {product.in_stock} (type: {type(product.in_stock).__name__})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedUserInput(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    query: str\n",
    "    order_id: Optional[int] = Field(\n",
    "        default=None,\n",
    "        description=\"5-digit order number, cannot start with 0\",\n",
    "        ge=10000,\n",
    "        le=99999\n",
    "    )\n",
    "    purchase_date: Optional[date] = None\n",
    "\n",
    "# From JSON string to model\n",
    "json_data = '''\n",
    "{\n",
    "    \"name\": \"Emily Rodriguez\",\n",
    "    \"email\": \"emily.r@example.com\",\n",
    "    \"query\": \"Need to update my shipping address\",\n",
    "    \"order_id\": 54321,\n",
    "    \"purchase_date\": \"2024-03-10\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Method 1: Parse JSON, then create model\n",
    "data_dict = json.loads(json_data)\n",
    "user1 = EnhancedUserInput(**data_dict)\n",
    "print(\"Method 1 - From dict:\")\n",
    "print(user1)\n",
    "\n",
    "# Method 2: Direct validation from JSON (preferred)\n",
    "user2 = EnhancedUserInput.model_validate_json(json_data)\n",
    "print(\"\\nMethod 2 - Direct from JSON:\")\n",
    "print(user2)\n",
    "\n",
    "# To JSON\n",
    "json_output = user2.model_dump_json(indent=2)\n",
    "print(\"\\nBack to JSON:\")\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validating LLM Responses\n",
    "\n",
    "Now let's see how to use Pydantic to validate and structure LLM outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Moderation System Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeverityLevel(str, Enum):\n",
    "    SAFE = \"safe\"\n",
    "    LOW = \"low\"\n",
    "    MEDIUM = \"medium\"\n",
    "    HIGH = \"high\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "class ViolationType(str, Enum):\n",
    "    NONE = \"none\"\n",
    "    SPAM = \"spam\"\n",
    "    HARASSMENT = \"harassment\"\n",
    "    HATE_SPEECH = \"hate_speech\"\n",
    "    EXPLICIT_CONTENT = \"explicit_content\"\n",
    "    MISINFORMATION = \"misinformation\"\n",
    "    VIOLENCE = \"violence\"\n",
    "\n",
    "class ModerationResult(BaseModel):\n",
    "    content_id: str\n",
    "    is_safe: bool\n",
    "    severity: SeverityLevel\n",
    "    violations: List[ViolationType] = Field(default_factory=list)\n",
    "    confidence_score: float = Field(ge=0.0, le=1.0)\n",
    "    flagged_phrases: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        max_items=10,\n",
    "        description=\"Specific phrases that triggered flags\"\n",
    "    )\n",
    "    recommended_action: Literal[\"approve\", \"review\", \"reject\", \"escalate\"]\n",
    "    explanation: str = Field(\n",
    "        min_length=20,\n",
    "        max_length=500,\n",
    "        description=\"Brief explanation of the decision\"\n",
    "    )\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"content_id\": \"post_12345\",\n",
    "                \"is_safe\": False,\n",
    "                \"severity\": \"medium\",\n",
    "                \"violations\": [\"spam\", \"misinformation\"],\n",
    "                \"confidence_score\": 0.87,\n",
    "                \"flagged_phrases\": [\"guaranteed results\", \"click here now\"],\n",
    "                \"recommended_action\": \"review\",\n",
    "                \"explanation\": \"Content contains promotional language and unverified claims requiring manual review\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Display the schema\n",
    "schema = ModerationResult.model_json_schema()\n",
    "print(\"ModerationResult Schema:\")\n",
    "print(json.dumps(schema, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Function with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_moderation_response(llm_response: str) -> tuple[ModerationResult | None, str | None]:\n",
    "    \"\"\"Validate LLM moderation response with detailed error handling.\"\"\"\n",
    "    try:\n",
    "        # First attempt: direct validation\n",
    "        result = ModerationResult.model_validate_json(llm_response)\n",
    "        return result, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, f\"Invalid JSON format: {str(e)}\"\n",
    "    except ValidationError as e:\n",
    "        # Extract specific validation errors\n",
    "        error_details = []\n",
    "        for error in e.errors():\n",
    "            field = \" -> \".join(str(x) for x in error[\"loc\"])\n",
    "            message = error[\"msg\"]\n",
    "            error_details.append(f\"{field}: {message}\")\n",
    "        return None, \"; \".join(error_details)\n",
    "\n",
    "# Test with valid data\n",
    "valid_response = '''\n",
    "{\n",
    "    \"content_id\": \"post_12345\",\n",
    "    \"is_safe\": false,\n",
    "    \"severity\": \"medium\",\n",
    "    \"violations\": [\"spam\"],\n",
    "    \"confidence_score\": 0.85,\n",
    "    \"flagged_phrases\": [\"buy now\", \"limited time\"],\n",
    "    \"recommended_action\": \"review\",\n",
    "    \"explanation\": \"Content contains promotional language that may violate spam policies\"\n",
    "}\n",
    "'''\n",
    "\n",
    "result, error = validate_moderation_response(valid_response)\n",
    "if result:\n",
    "    print(\"✓ Validation successful!\")\n",
    "    print(f\"Content ID: {result.content_id}\")\n",
    "    print(f\"Safe: {result.is_safe}\")\n",
    "    print(f\"Action: {result.recommended_action}\")\n",
    "else:\n",
    "    print(f\"✗ Validation failed: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with LLM - Content Moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_moderation_prompt(content: str, content_id: str) -> str:\n",
    "    \"\"\"Create a schema-based prompt for content moderation.\"\"\"\n",
    "    schema = ModerationResult.model_json_schema()\n",
    "    \n",
    "    prompt = f\"\"\"You are a content moderation AI. Analyze the following content for policy violations.\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "ID: {content_id}\n",
    "Text: {content}\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "Provide your analysis as a valid JSON object that strictly conforms to this schema:\n",
    "\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Return ONLY valid JSON - no markdown formatting, no explanatory text\n",
    "2. Ensure all required fields are present\n",
    "3. Follow exact field types and constraints\n",
    "4. confidence_score must be between 0.0 and 1.0\n",
    "5. severity must be one of: safe, low, medium, high, critical\n",
    "\n",
    "Begin your response with {{ and end with }}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Test content\n",
    "test_content = \"Click here now for guaranteed weight loss! Limited time offer!\"\n",
    "content_id = \"test_001\"\n",
    "\n",
    "prompt = create_moderation_prompt(test_content, content_id)\n",
    "print(\"Prompt sent to LLM:\")\n",
    "print(\"=\" * 60)\n",
    "print(prompt)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LLM response\n",
    "response = llm.invoke(prompt)\n",
    "llm_output = response.content\n",
    "\n",
    "print(\"\\nLLM Response:\")\n",
    "print(\"=\" * 60)\n",
    "print(llm_output)\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validate the response\n",
    "result, error = validate_moderation_response(llm_output)\n",
    "\n",
    "if result:\n",
    "    print(\"\\n✓ Validation SUCCESSFUL!\")\n",
    "    print(\"\\nParsed Result:\")\n",
    "    print(json.dumps(result.model_dump(), indent=2, default=str))\n",
    "else:\n",
    "    print(f\"\\n✗ Validation FAILED: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retry Logic with Error Feedback\n",
    "\n",
    "LLMs don't always get the format right on the first try. Let's implement a retry mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_llm_response_with_retry(\n",
    "    prompt: str,\n",
    "    data_model: type[BaseModel],\n",
    "    max_retries: int = 3,\n",
    ") -> BaseModel | None:\n",
    "    \"\"\"\n",
    "    Validate LLM response with automatic retry on validation errors.\n",
    "    \"\"\"\n",
    "    # Initial call\n",
    "    response = llm.invoke(prompt)\n",
    "    llm_response = response.content\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Attempt validation\n",
    "            validated_data = data_model.model_validate_json(llm_response)\n",
    "            print(f\"✓ Validation successful on attempt {attempt + 1}\")\n",
    "            return validated_data\n",
    "            \n",
    "        except (ValidationError, json.JSONDecodeError) as e:\n",
    "            print(f\"✗ Attempt {attempt + 1} failed: {str(e)[:200]}...\")\n",
    "            \n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"Max retries reached. Validation failed.\")\n",
    "                return None\n",
    "            \n",
    "            # Create retry prompt with error feedback\n",
    "            retry_prompt = f\"\"\"VALIDATION ERROR OCCURRED\n",
    "\n",
    "Original Prompt:\n",
    "{prompt}\n",
    "\n",
    "Your Previous Response:\n",
    "{llm_response}\n",
    "\n",
    "Error Message:\n",
    "{str(e)}\n",
    "\n",
    "Please fix the error and provide a corrected response. Remember:\n",
    "- Return ONLY valid JSON\n",
    "- No markdown formatting or extra text\n",
    "- Match the exact schema requirements\n",
    "- Begin with {{ and end with }}\"\"\"\n",
    "            \n",
    "            # Retry with error feedback\n",
    "            response = llm.invoke(retry_prompt)\n",
    "            llm_response = response.content\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test with retry logic\n",
    "print(\"Testing retry logic...\\n\")\n",
    "result = validate_llm_response_with_retry(\n",
    "    prompt=create_moderation_prompt(test_content, content_id),\n",
    "    data_model=ModerationResult,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\nFinal validated result:\")\n",
    "    print(json.dumps(result.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Example: Research Paper Analysis\n",
    "\n",
    "Let's build a more complex model for analyzing academic papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(BaseModel):\n",
    "    name: str\n",
    "    affiliation: str\n",
    "    email: Optional[EmailStr] = None\n",
    "\n",
    "class ResearchPaperAnalysis(BaseModel):\n",
    "    title: str = Field(min_length=10, max_length=300)\n",
    "    authors: List[Author] = Field(min_items=1, max_items=20)\n",
    "    publication_date: date\n",
    "    abstract: str = Field(min_length=100, max_length=2000)\n",
    "    keywords: List[str] = Field(min_items=3, max_items=10)\n",
    "    methodology: str = Field(min_length=50, max_length=1000)\n",
    "    key_findings: List[str] = Field(\n",
    "        min_items=2,\n",
    "        max_items=5,\n",
    "        description=\"Main discoveries or conclusions\"\n",
    "    )\n",
    "    limitations: List[str] = Field(min_items=1, max_items=5)\n",
    "    impact_score: float = Field(\n",
    "        ge=0.0,\n",
    "        le=10.0,\n",
    "        description=\"Estimated research impact (0-10)\"\n",
    "    )\n",
    "    citations_analyzed: int = Field(ge=0)\n",
    "    related_works: List[HttpUrl] = Field(default_factory=list, max_items=10)\n",
    "\n",
    "# Display schema\n",
    "print(\"Research Paper Analysis Schema:\")\n",
    "print(json.dumps(ResearchPaperAnalysis.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_analysis_prompt(paper_abstract: str) -> str:\n",
    "    \"\"\"Create a schema-based prompt for paper analysis.\"\"\"\n",
    "    schema = ResearchPaperAnalysis.model_json_schema()\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert research analyst. Analyze the following academic paper abstract and extract structured information.\n",
    "\n",
    "PAPER ABSTRACT:\n",
    "{paper_abstract}\n",
    "\n",
    "OUTPUT REQUIREMENTS:\n",
    "Provide your analysis as a valid JSON object that strictly conforms to this schema:\n",
    "\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Return ONLY valid JSON - no markdown formatting, no explanatory text\n",
    "2. Ensure all required fields are present\n",
    "3. Follow exact field types and constraints\n",
    "4. Arrays must contain the specified minimum number of items\n",
    "5. Dates must be in YYYY-MM-DD format\n",
    "6. Make reasonable inferences based on the abstract\n",
    "7. For missing information, use plausible estimates\n",
    "\n",
    "Begin your response with {{ and end with }}\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Sample abstract\n",
    "sample_abstract = \"\"\"Large language models have demonstrated remarkable capabilities in natural language understanding\n",
    "and generation. This paper presents a novel approach to improving structured output generation from LLMs using\n",
    "validation frameworks. We evaluated our method on three benchmark datasets and achieved a 34% reduction in\n",
    "output formatting errors compared to baseline approaches. Our findings suggest that incorporating runtime\n",
    "validation significantly improves the reliability of LLM-powered applications in production environments.\"\"\"\n",
    "\n",
    "prompt = create_paper_analysis_prompt(sample_abstract)\n",
    "result = validate_llm_response_with_retry(\n",
    "    prompt=prompt,\n",
    "    data_model=ResearchPaperAnalysis,\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "if result:\n",
    "    print(\"\\n✓ Paper Analysis Completed!\")\n",
    "    print(\"\\nStructured Output:\")\n",
    "    print(json.dumps(result.model_dump(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Patterns\n",
    "\n",
    "Let's explore some important patterns for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Validators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transaction(BaseModel):\n",
    "    transaction_id: str\n",
    "    amount: float\n",
    "    currency: str\n",
    "    timestamp: datetime\n",
    "    \n",
    "    @validator('amount')\n",
    "    def amount_must_be_positive(cls, v):\n",
    "        if v <= 0:\n",
    "            raise ValueError('Amount must be positive')\n",
    "        return v\n",
    "    \n",
    "    @validator('currency')\n",
    "    def currency_must_be_valid(cls, v):\n",
    "        valid_currencies = {'USD', 'EUR', 'GBP', 'JPY'}\n",
    "        if v.upper() not in valid_currencies:\n",
    "            raise ValueError(f'Currency must be one of {valid_currencies}')\n",
    "        return v.upper()\n",
    "\n",
    "# Test valid transaction\n",
    "txn = Transaction(\n",
    "    transaction_id=\"TXN123\",\n",
    "    amount=99.99,\n",
    "    currency=\"usd\",\n",
    "    timestamp=datetime.now()\n",
    ")\n",
    "print(f\"Valid transaction: {txn.currency} {txn.amount}\")\n",
    "\n",
    "# Test invalid amount\n",
    "try:\n",
    "    invalid_txn = Transaction(\n",
    "        transaction_id=\"TXN124\",\n",
    "        amount=-50.0,\n",
    "        currency=\"USD\",\n",
    "        timestamp=datetime.now()\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"\\nValidation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseUserInfo(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "    user_id: str = Field(pattern=r\"^USR\\d{6}$\")\n",
    "\n",
    "class UserRegistration(BaseUserInfo):\n",
    "    password: str = Field(min_length=8, max_length=128)\n",
    "    confirm_password: str\n",
    "    terms_accepted: bool = True\n",
    "    \n",
    "    @validator('confirm_password')\n",
    "    def passwords_match(cls, v, values):\n",
    "        if 'password' in values and v != values['password']:\n",
    "            raise ValueError('Passwords do not match')\n",
    "        return v\n",
    "\n",
    "class UserProfile(BaseUserInfo):\n",
    "    bio: Optional[str] = Field(default=None, max_length=500)\n",
    "    avatar_url: Optional[HttpUrl] = None\n",
    "    created_at: datetime\n",
    "    last_login: Optional[datetime] = None\n",
    "\n",
    "# Create instances\n",
    "registration = UserRegistration(\n",
    "    name=\"John Doe\",\n",
    "    email=\"john@example.com\",\n",
    "    user_id=\"USR123456\",\n",
    "    password=\"SecurePass123\",\n",
    "    confirm_password=\"SecurePass123\"\n",
    ")\n",
    "\n",
    "profile = UserProfile(\n",
    "    name=\"John Doe\",\n",
    "    email=\"john@example.com\",\n",
    "    user_id=\"USR123456\",\n",
    "    created_at=datetime.now(),\n",
    "    bio=\"Software developer\"\n",
    ")\n",
    "\n",
    "print(\"Registration:\", registration.name)\n",
    "print(\"Profile:\", profile.name, \"-\", profile.bio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe Validation Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "def safe_validate(\n",
    "    data: Union[str, dict],\n",
    "    model: type[BaseModel]\n",
    ") -> tuple[BaseModel | None, dict]:\n",
    "    \"\"\"\n",
    "    Safely validate data with detailed error information.\n",
    "    \n",
    "    Returns:\n",
    "        (validated_model, error_info) where error_info is empty dict if successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(data, str):\n",
    "            validated = model.model_validate_json(data)\n",
    "        else:\n",
    "            validated = model.model_validate(data)\n",
    "        return validated, {}\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, {\n",
    "            \"error_type\": \"json_decode\",\n",
    "            \"message\": str(e),\n",
    "            \"position\": e.pos\n",
    "        }\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        return None, {\n",
    "            \"error_type\": \"validation\",\n",
    "            \"errors\": [\n",
    "                {\n",
    "                    \"field\": \".\".join(str(x) for x in err[\"loc\"]),\n",
    "                    \"message\": err[\"msg\"],\n",
    "                    \"type\": err[\"type\"]\n",
    "                }\n",
    "                for err in e.errors()\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Test the helper\n",
    "test_data = '{\"name\": \"Test\", \"email\": \"invalid-email\", \"query\": \"Help\"}'\n",
    "result, error_info = safe_validate(test_data, UserInput)\n",
    "\n",
    "if result:\n",
    "    print(\"✓ Validation successful\")\n",
    "else:\n",
    "    print(\"✗ Validation failed:\")\n",
    "    print(json.dumps(error_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways and Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Start with Clear Models**: Well-defined Pydantic models are the foundation of reliable LLM integrations\n",
    "\n",
    "2. **Use Schema Over Examples**: Pass `model_json_schema()` to LLMs for better structured output\n",
    "\n",
    "3. **Implement Retry Logic**: LLMs don't always get it right the first time; build in error handling and retries\n",
    "\n",
    "4. **Validate Early and Often**: Catch data issues as early as possible in your pipeline\n",
    "\n",
    "5. **Leverage Type Safety**: Pydantic's type checking prevents many runtime errors\n",
    "\n",
    "6. **Document Your Models**: Use `Field(description=...)` to make models self-documenting\n",
    "\n",
    "7. **Test Thoroughly**: Write tests for both valid and invalid data scenarios\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Modern LLM APIs (like OpenAI) support passing Pydantic models directly for structured outputs\n",
    "- Explore instructor library for even more streamlined LLM + Pydantic workflows\n",
    "- Build production-ready validation pipelines with comprehensive error handling\n",
    "- Consider using Pydantic v2 for improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create a model for a movie review\n",
    "# Requirements:\n",
    "# - movie_title: string\n",
    "# - reviewer_name: string\n",
    "# - rating: float between 0 and 5\n",
    "# - review_text: string (min 50 chars)\n",
    "# - watched_date: date\n",
    "# - would_recommend: boolean\n",
    "# - genre: one of [\"action\", \"comedy\", \"drama\", \"horror\", \"sci-fi\"]\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a prompt for the LLM to analyze a movie review\n",
    "# and validate the response using your model\n",
    "\n",
    "sample_review = \"\"\"I watched The Matrix last night and it was absolutely mind-blowing! \n",
    "The special effects still hold up today, and the philosophical themes are fascinating. \n",
    "Definitely a must-watch for any sci-fi fan. Rating: 4.5/5\"\"\"\n",
    "\n",
    "# Your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}