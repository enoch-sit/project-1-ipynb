{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# LangChain Tutorial: Key Concepts Explained\n",
        "\n",
        "This notebook provides a step-by-step tutorial on the core concepts of LangChain, based on our discussion. We'll cover LCEL (LangChain Expression Language), Runnables, components like prompts and models, and how the pipe operator works through Python's operator overloading.\n",
        "\n",
        "We'll use Azure OpenAI's GPT-4o-mini model via the provided setup code. Make sure to run the setup cell first and have your API key stored in Google Colab's userdata.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Install required packages: `!pip install langchain langchain-openai langchain-community langchain-core docarray hnswlib`\n",
        "- Store your Azure OpenAI key in Colab secrets as 'eduhkkey'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai langchain-community langchain-core langchain-huggingface docarray hnswlib sentence-transformers -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Azure OpenAI Model\n",
        "\n",
        "This cell sets up the AzureChatOpenAI model using the provided code with the correct endpoint URL including the ?Hello= parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set your Azure OpenAI API key (keep it secret! In Colab, you can use os.environ for security)\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('eduhkkey')\n",
        "\n",
        "# Set up the Azure OpenAI model (using gpt-4o-mini as per docs)\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",  # Use a recent version\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0,  # Low temperature for consistent tool calling\n",
        "    streaming=False,  # Non-streaming for simplicity\n",
        ")\n",
        "\n",
        "# The actual endpoint used internally\n",
        "print(f\"Base URL: {llm.client._client._base_url}\")\n",
        "print(f\"API Version: {llm.openai_api_version}\")\n",
        "print(f\"Deployment: {llm.deployment_name}\")\n",
        "print(os.environ[\"AZURE_OPENAI_API_KEY\"])  # This will print the keyâ€”remove in production!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Introduction to LCEL and Runnable Protocol\n",
        "\n",
        "LCEL (LangChain Expression Language) is a declarative way to compose chains of components in LangChain. It uses the Runnable Protocol, which defines standardized methods (invoke, stream, batch) that all components must implement.\n",
        "\n",
        "Key: Components like prompts and models are 'Runnables' that can be chained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Simple prompt template (Runnable)\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Chain with pipe syntax (LCEL)\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke (sync, single)\n",
        "result = chain.invoke({\"topic\": \"cats\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Runnable Interface and Methods\n",
        "\n",
        "Every Runnable supports:\n",
        "- invoke/ainvoke: Sync/async single input.\n",
        "- batch/abatch: Process multiple inputs.\n",
        "- stream/astream: Incremental output.\n",
        "- input_schema/output_schema: Define I/O types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch example\n",
        "inputs = [{\"topic\": \"cats\"}, {\"topic\": \"dogs\"}]\n",
        "results = chain.batch(inputs)\n",
        "print(results)\n",
        "\n",
        "# Stream example\n",
        "for chunk in chain.stream({\"topic\": \"birds\"}):\n",
        "    print(chunk, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Advanced Features - Fallbacks, Parallelism, Logging\n",
        "\n",
        "Fallbacks: Add backups with .with_fallbacks().\n",
        "Parallelism: Use RunnableParallel/RunnableMap for concurrent steps.\n",
        "Logging: Built-in to LangSmith (setup required)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "# Parallel chain\n",
        "parallel_chain = RunnableParallel(\n",
        "    joke=chain,\n",
        "    fact=prompt | llm | StrOutputParser()  # Reuse\n",
        ")\n",
        "\n",
        "result = parallel_chain.invoke({\"topic\": \"space\"})\n",
        "print(result)\n",
        "\n",
        "# Fallback example (using a secondary model if primary fails)\n",
        "fallback_llm = AzureChatOpenAI(  # Another instance as fallback\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0.5,\n",
        "    max_retries=0  # Immediate fallback\n",
        ")\n",
        "chain_with_fallback = llm.with_fallbacks([fallback_llm])\n",
        "result = (prompt | chain_with_fallback | StrOutputParser()).invoke({\"topic\": \"fallback test\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Pipe Syntax vs. RunnableSequence\n",
        "\n",
        "Pipe (`|`) is shorthand for RunnableSequence via operator overloading (__or__). Verbose alternative: Explicitly use RunnableSequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "\n",
        "# Pipe syntax\n",
        "pipe_chain = prompt | llm\n",
        "\n",
        "# Verbose equivalent\n",
        "sequence_chain = RunnableSequence(prompt, llm)\n",
        "\n",
        "# Both work the same\n",
        "print(pipe_chain.invoke({\"topic\": \"verbose\"}))\n",
        "print(sequence_chain.invoke({\"topic\": \"verbose\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Understanding the Pipe Operator (|) and Operator Overloading\n",
        "\n",
        "The `|` symbol in Python is natively the bitwise OR operator (for integers) or set/dict union operator (in Python 3.9+), but libraries like LangChain use a clever (but fully legitimate) technique called **operator overloading** to repurpose it as a \"pipe\" for composing objects, mimicking the Linux/Unix shell pipe (`|`) that chains commands.\n",
        "\n",
        "### How It Works in Python\n",
        "- **Native Behavior**: Without overloading, `a | b` does bitwise OR if `a` and `b` are ints (e.g., `5 | 3` is 7), or unions sets/dicts (e.g., `{\"a\":1} | {\"b\":2}` is `{\"a\":1, \"b\":2}`).\n",
        "- **Overloading Trick**: Python classes can define special methods (dunder methods) to customize operators. For `|`, it's `__or__` (and optionally `__ror__` for reverse). If you implement this in a class, `obj1 | obj2` calls `obj1.__or__(obj2)`, letting you define custom behavior like chaining.\n",
        "- **In LangChain's LCEL**: The `Runnable` class overloads `__or__` to create a `RunnableSequence`. So `prompt | model` returns a new object that pipes the output of `prompt` into `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's demonstrate operator overloading with a simple example\n",
        "class SimplePipe:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    \n",
        "    def __or__(self, other):\n",
        "        # This is what happens when you use | operator\n",
        "        def chained(x):\n",
        "            return other.func(self.func(x))\n",
        "        return SimplePipe(chained)\n",
        "    \n",
        "    def invoke(self, x):\n",
        "        return self.func(x)\n",
        "\n",
        "# Create simple pipe components\n",
        "add_one = SimplePipe(lambda x: x + 1)\n",
        "double = SimplePipe(lambda x: x * 2)\n",
        "\n",
        "# Chain them with | operator (this calls add_one.__or__(double))\n",
        "chain = add_one | double  # Overloads | to chain functions\n",
        "result = chain.invoke(5)  # (5 + 1) * 2 = 12\n",
        "print(f\"Result: {result}\")  # Output: 12\n",
        "\n",
        "# Show what happens under the hood\n",
        "print(f\"Type of chain: {type(chain)}\")\n",
        "print(f\"Chain is a SimplePipe: {isinstance(chain, SimplePipe)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why It Feels Like a Pipe\n",
        "\n",
        "- **Inspired by shells**: In Linux, `cmd1 | cmd2` sends output from cmd1 to cmd2 as input. LCEL does the same for data flow (e.g., prompt output â†’ model input).\n",
        "- **Pros**: Makes code concise and intuitive, especially for pipelines.\n",
        "- **Cons**: Can confuse beginners if they're expecting bitwise OR, but context (like importing LangChain) makes it clear.\n",
        "\n",
        "This has been standard in Python for decades and remains unchanged in 2025â€”it's not going anywhere. Let's see how LangChain implements this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate native Python operators vs LangChain overloading\n",
        "\n",
        "# Native bitwise OR\n",
        "print(\"Native bitwise OR:\")\n",
        "print(f\"5 | 3 = {5 | 3}\")  # Bitwise OR: 7\n",
        "\n",
        "# Native set union (Python 3.9+)\n",
        "print(\"\\nNative set/dict union:\")\n",
        "set1 = {1, 2, 3}\n",
        "set2 = {3, 4, 5}\n",
        "print(f\"{set1} | {set2} = {set1 | set2}\")\n",
        "\n",
        "dict1 = {\"a\": 1, \"b\": 2}\n",
        "dict2 = {\"b\": 3, \"c\": 4}\n",
        "print(f\"{dict1} | {dict2} = {dict1 | dict2}\")\n",
        "\n",
        "# LangChain overloaded behavior\n",
        "print(\"\\nLangChain overloaded | operator:\")\n",
        "simple_prompt = ChatPromptTemplate.from_template(\"Say hello to {name}\")\n",
        "chained = simple_prompt | llm\n",
        "print(f\"Type of result: {type(chained)}\")\n",
        "print(f\"Result: {chained.invoke({'name': 'Alice'})}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways\n",
        "\n",
        "1. **Not a hack**: Operator overloading is a core Python feature, like NumPy using `+` for array addition.\n",
        "2. **Intuitive design**: `prompt | model | parser` reads left-to-right like Unix pipes.\n",
        "3. **Under the hood**: `prompt | model` calls `prompt.__or__(model)` which returns a `RunnableSequence`.\n",
        "4. **Flexible**: You can implement this pattern in your own classes for domain-specific pipelines.\n",
        "\n",
        "This approach makes LangChain chains both powerful and readable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Data Flow in Chains\n",
        "\n",
        "In chains, output of one component becomes input to the next. E.g., Prompt output â†’ Model input â†’ Parser input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect flow\n",
        "prompt_output = prompt.invoke({\"topic\": \"flow\"})\n",
        "print(\"Prompt Output:\", prompt_output)\n",
        "\n",
        "model_output = llm.invoke(prompt_output)\n",
        "print(\"Model Output:\", model_output)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "final_output = parser.invoke(model_output)\n",
        "print(\"Final Output:\", final_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Switching Execution Modes\n",
        "\n",
        "Sync â†’ Async: Use ainvoke/astream/abatch.\n",
        "Single â†’ Batch: Pass list to batch/abatch.\n",
        "Non-stream â†’ Streaming: Use stream/astream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Redefine the LangChain chain (same as Section 1)\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "import asyncio\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Redefine the LangChain chain to avoid overwrite from Section 5\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Async invoke\n",
        "async def async_invoke():\n",
        "    return await chain.ainvoke({\"topic\": \"async\"})\n",
        "\n",
        "result = await async_invoke()\n",
        "print(result)\n",
        "\n",
        "# Async stream\n",
        "async def async_stream():\n",
        "    async for chunk in chain.astream({\"topic\": \"stream\"}):\n",
        "        print(chunk, end=\"@\")\n",
        "\n",
        "await async_stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: RAG Example with VectorStore and Retriever\n",
        "\n",
        "Build a Retrieval-Augmented Generation chain: Embed docs, retrieve relevant ones, augment prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.runnables import RunnableMap\n",
        "\n",
        "# Embeddings (use HuggingFace - free, local, no API key required)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"I am a superman\", \"This apple is great\"],\n",
        "    embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = RunnableMap({\n",
        "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
        "    \"question\": lambda x: x[\"question\"]\n",
        "}) | prompt | llm | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"question\": \"Who am I?\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Handling Custom Endpoints Without Native Binding\n",
        "\n",
        "If the endpoint lacks function binding, use prompt engineering: Instruct the model to output JSON tool calls, parse, and invoke manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.tools import render_text_description\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "@tool\n",
        "def weather_search(airport_code: str) -> str:\n",
        "    \"\"\"Search for weather given an airport code.\"\"\"\n",
        "    return f\"Weather for {airport_code}: Sunny, 75Â°F\"\n",
        "\n",
        "tools = [weather_search]\n",
        "rendered_tools = render_text_description(tools)\n",
        "\n",
        "system_prompt = f\"\"\"You are an assistant with access to tools.\n",
        "{rendered_tools}\n",
        "\n",
        "If relevant, return JSON with 'name' and 'arguments'.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_prompt),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "parser = JsonOutputParser()\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "def invoke_tool(tool_call: dict):\n",
        "    tool_name = tool_call.get(\"name\")\n",
        "    tool_args = tool_call.get(\"arguments\", {})\n",
        "    for t in tools:\n",
        "        if t.name == tool_name:\n",
        "            return t.invoke(tool_args)\n",
        "    raise ValueError(\"Tool not found\")\n",
        "\n",
        "full_chain = chain | RunnableLambda(invoke_tool)\n",
        "result = full_chain.invoke({\"input\": \"What's the weather at SFO?\"})\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
