{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic with Hugging Face Models for Structured Outputs\n",
    "\n",
    "This tutorial demonstrates how to implement structured data validation and tool calling with Pydantic using affordable/free Hugging Face models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll build a customer support system that:\n",
    "1. Validates user input with Pydantic\n",
    "2. Generates structured customer queries\n",
    "3. Performs tool calling with validation\n",
    "4. Creates final support tickets\n",
    "\n",
    "**Key difference:** We'll use free Hugging Face models (like Mistral-7B or Llama-3-8B) via the Hugging Face Inference API instead of paid APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pydantic huggingface_hub requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Optional, Literal\n",
    "import json\n",
    "import re\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationError\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Hugging Face\n",
    "\n",
    "Get your free API token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Then either:\n",
    "- Set it as an environment variable: `export HF_TOKEN=your_token_here`\n",
    "- Or paste it directly in the cell below (not recommended for shared notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Get from environment variable\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Option 2: If not set as environment variable, uncomment and paste your token here:\n",
    "# HF_TOKEN = \"your_token_here\"\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Please set your HF_TOKEN either as environment variable or in the cell above\")\n",
    "\n",
    "# Initialize the Hugging Face client\n",
    "client = InferenceClient(token=HF_TOKEN)\n",
    "\n",
    "# We'll use Mistral-7B-Instruct - it's free and good at following instructions\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Alternative free models you can try:\n",
    "# \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "print(f\"✓ Configured to use model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Pydantic Models\n",
    "\n",
    "These models define the structure and validation rules for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base User Input Model with custom validation\n",
    "class UserInput(BaseModel):\n",
    "    name: str = Field(..., description=\"Customer's full name\")\n",
    "    email: str = Field(..., pattern=r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', description=\"Valid email address\")\n",
    "    message: str = Field(..., min_length=5, description=\"Customer's message\")\n",
    "    order_id: Optional[str] = Field(None, description=\"Order ID format: ABC-12345 (3 letters, dash, 5 numbers)\")\n",
    "    \n",
    "    @field_validator('order_id')\n",
    "    @classmethod\n",
    "    def validate_order_id(cls, v):\n",
    "        if v is None:\n",
    "            return v\n",
    "        pattern = r'^[A-Z]{3}-\\d{5}$'\n",
    "        if not re.match(pattern, v):\n",
    "            raise ValueError(f\"Order ID must match format ABC-12345. Got: {v}\")\n",
    "        return v\n",
    "\n",
    "# Customer Query Model (extends UserInput)\n",
    "class CustomerQuery(UserInput):\n",
    "    category: Literal[\"order_status\", \"product_question\", \"complaint\", \"password_reset\"] = Field(\n",
    "        ..., description=\"Category of customer query\"\n",
    "    )\n",
    "    urgency: Literal[\"low\", \"medium\", \"high\"] = Field(\n",
    "        ..., description=\"Urgency level of the query\"\n",
    "    )\n",
    "    sentiment: Literal[\"positive\", \"neutral\", \"negative\"] = Field(\n",
    "        ..., description=\"Sentiment of the customer message\"\n",
    "    )\n",
    "    tags: list[str] = Field(\n",
    "        default_factory=list, description=\"Relevant tags for categorization\"\n",
    "    )\n",
    "\n",
    "# Tool argument models\n",
    "class FAQLookupArgs(BaseModel):\n",
    "    query: str = Field(..., description=\"Search query for FAQ lookup\")\n",
    "    tags: list[str] = Field(default_factory=list, description=\"Tags to filter FAQ results\")\n",
    "\n",
    "class CheckOrderStatusArgs(BaseModel):\n",
    "    order_id: str = Field(..., description=\"Order ID format: ABC-12345\")\n",
    "    email: str = Field(..., description=\"Customer email for verification\")\n",
    "    \n",
    "    @field_validator('order_id')\n",
    "    @classmethod\n",
    "    def validate_order_id(cls, v):\n",
    "        pattern = r'^[A-Z]{3}-\\d{5}$'\n",
    "        if not re.match(pattern, v):\n",
    "            raise ValueError(f\"Order ID must match format ABC-12345. Got: {v}\")\n",
    "        return v\n",
    "\n",
    "# Final output model\n",
    "class OrderDetails(BaseModel):\n",
    "    status: str\n",
    "    estimated_delivery: Optional[str] = None\n",
    "    note: Optional[str] = None\n",
    "\n",
    "class SupportTicket(CustomerQuery):\n",
    "    recommended_next_action: Literal[\"escalate_to_agent\", \"send_faq_response\", \"send_order_status\", \"no_action_needed\"]\n",
    "    order_details: Optional[OrderDetails] = None\n",
    "    faq_response: Optional[str] = None\n",
    "    creation_date: Optional[str] = None\n",
    "\n",
    "print(\"✓ Pydantic models defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Helper Functions for HuggingFace LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_hf_model(prompt: str, model_name: str = MODEL_NAME, max_tokens: int = 1000, temperature: float = 0.3):\n",
    "    \"\"\"\n",
    "    Call Hugging Face model and return response.\n",
    "    Lower temperature for more consistent structured outputs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        response = client.chat_completion(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling model: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_json_from_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract JSON from model response that might contain additional text.\n",
    "    \"\"\"\n",
    "    # Try to find JSON between code blocks\n",
    "    if \"```json\" in response:\n",
    "        start = response.find(\"```json\") + 7\n",
    "        end = response.find(\"```\", start)\n",
    "        return response[start:end].strip()\n",
    "    elif \"```\" in response:\n",
    "        start = response.find(\"```\") + 3\n",
    "        end = response.find(\"```\", start)\n",
    "        return response[start:end].strip()\n",
    "    \n",
    "    # Try to find JSON object\n",
    "    try:\n",
    "        start = response.find(\"{\")\n",
    "        end = response.rfind(\"}\") + 1\n",
    "        if start != -1 and end > start:\n",
    "            return response[start:end]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Validate User Input\n",
    "\n",
    "Let's test our validation with a sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_input(user_input_json: str) -> Optional[UserInput]:\n",
    "    \"\"\"\n",
    "    Validate raw user input against UserInput model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        user_input = UserInput.model_validate_json(user_input_json)\n",
    "        print(\"✓ User input validated successfully\")\n",
    "        return user_input\n",
    "    except ValidationError as e:\n",
    "        print(\"✗ Validation Error:\")\n",
    "        for error in e.errors():\n",
    "            print(f\"  - {error['loc'][0]}: {error['msg']}\")\n",
    "        return None\n",
    "\n",
    "# Test with valid input\n",
    "user_input_json = json.dumps({\n",
    "    \"name\": \"Jane Smith\",\n",
    "    \"email\": \"jane.smith@example.com\",\n",
    "    \"message\": \"What is the status of my order?\",\n",
    "    \"order_id\": \"ABC-12345\"\n",
    "})\n",
    "\n",
    "validated_input = validate_user_input(user_input_json)\n",
    "if validated_input:\n",
    "    print(f\"\\nValidated data:\\n{validated_input.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Customer Query with LLM\n",
    "\n",
    "Use the LLM to analyze the user input and generate a structured CustomerQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_customer_query(validated_user_input: UserInput, max_retries: int = 3) -> Optional[CustomerQuery]:\n",
    "    \"\"\"\n",
    "    Use LLM to analyze user input and create a CustomerQuery with retries.\n",
    "    \"\"\"\n",
    "    schema = CustomerQuery.model_json_schema()\n",
    "    \n",
    "    prompt = f\"\"\"You are a customer support AI. Analyze the following customer input and return ONLY a JSON object matching this schema:\n",
    "\n",
    "Schema: {json.dumps(schema, indent=2)}\n",
    "\n",
    "Customer Input:\n",
    "{validated_user_input.model_dump_json(indent=2)}\n",
    "\n",
    "Requirements:\n",
    "- Return ONLY valid JSON, no additional text\n",
    "- Include all required fields\n",
    "- Choose appropriate category, urgency, and sentiment\n",
    "- Add relevant tags\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nAttempt {attempt + 1} to generate CustomerQuery...\")\n",
    "            response = call_hf_model(prompt, temperature=0.2)\n",
    "            \n",
    "            if not response:\n",
    "                continue\n",
    "            \n",
    "            # Extract JSON from response\n",
    "            json_str = extract_json_from_response(response)\n",
    "            print(f\"Extracted JSON: {json_str[:200]}...\")\n",
    "            \n",
    "            # Validate with Pydantic\n",
    "            customer_query = CustomerQuery.model_validate_json(json_str)\n",
    "            print(\"✓ CustomerQuery generated and validated successfully\")\n",
    "            return customer_query\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            print(f\"✗ Validation error on attempt {attempt + 1}:\")\n",
    "            for error in e.errors():\n",
    "                print(f\"  - {error['loc']}: {error['msg']}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"✗ JSON decode error: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test it\n",
    "customer_query = create_customer_query(validated_input)\n",
    "if customer_query:\n",
    "    print(f\"\\nFinal CustomerQuery:\\n{customer_query.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set Up Mock Databases\n",
    "\n",
    "Create mock databases for FAQ and orders to simulate real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock FAQ Database\n",
    "FAQ_DATABASE = [\n",
    "    {\n",
    "        \"question\": \"How do I reset my password?\",\n",
    "        \"answer\": \"Visit the 'Forgot Password' page, enter your email, and follow the reset link sent to you.\",\n",
    "        \"keywords\": [\"password\", \"reset\", \"login\", \"access\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is your return policy?\",\n",
    "        \"answer\": \"We offer 30-day returns for unopened items with receipt. Contact support to initiate.\",\n",
    "        \"keywords\": [\"return\", \"refund\", \"policy\", \"money back\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How long does shipping take?\",\n",
    "        \"answer\": \"Standard shipping takes 5-7 business days. Express shipping is 2-3 business days.\",\n",
    "        \"keywords\": [\"shipping\", \"delivery\", \"how long\", \"tracking\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Mock Order Database\n",
    "ORDER_DATABASE = {\n",
    "    \"ABC-12345\": {\n",
    "        \"status\": \"In Transit\",\n",
    "        \"estimated_delivery\": \"2025-10-08\",\n",
    "        \"purchase_date\": \"2025-10-01\",\n",
    "        \"email\": \"jane.smith@example.com\"\n",
    "    },\n",
    "    \"XYZ-98765\": {\n",
    "        \"status\": \"Delivered\",\n",
    "        \"estimated_delivery\": \"2025-10-02\",\n",
    "        \"purchase_date\": \"2025-09-25\",\n",
    "        \"email\": \"joe.user@example.com\"\n",
    "    },\n",
    "    \"DEF-55555\": {\n",
    "        \"status\": \"Processing\",\n",
    "        \"estimated_delivery\": \"2025-10-10\",\n",
    "        \"purchase_date\": \"2025-10-03\",\n",
    "        \"email\": \"bob.jones@example.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Mock databases created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Define Tool Functions\n",
    "\n",
    "These are the actual functions that will be called based on LLM decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_faq_answer(args: FAQLookupArgs) -> str:\n",
    "    \"\"\"\n",
    "    Search FAQ database for relevant answers.\n",
    "    \"\"\"\n",
    "    query_lower = args.query.lower()\n",
    "    tags_lower = [tag.lower() for tag in args.tags]\n",
    "    \n",
    "    for faq in FAQ_DATABASE:\n",
    "        # Check if query matches keywords or tags match\n",
    "        keywords_match = any(keyword in query_lower for keyword in faq[\"keywords\"])\n",
    "        tags_match = any(tag in faq[\"keywords\"] for tag in tags_lower)\n",
    "        \n",
    "        if keywords_match or tags_match:\n",
    "            return f\"FAQ Answer: {faq['answer']}\"\n",
    "    \n",
    "    return \"Sorry, I couldn't find an FAQ answer for your question.\"\n",
    "\n",
    "def check_order_status(args: CheckOrderStatusArgs) -> dict:\n",
    "    \"\"\"\n",
    "    Look up order status from database.\n",
    "    \"\"\"\n",
    "    order = ORDER_DATABASE.get(args.order_id)\n",
    "    \n",
    "    if not order:\n",
    "        return {\"error\": \"Order not found\", \"order_id\": args.order_id}\n",
    "    \n",
    "    if order[\"email\"] != args.email:\n",
    "        return {\"error\": \"Email does not match order records\", \"order_id\": args.order_id}\n",
    "    \n",
    "    return {\n",
    "        \"order_id\": args.order_id,\n",
    "        \"status\": order[\"status\"],\n",
    "        \"estimated_delivery\": order[\"estimated_delivery\"],\n",
    "        \"note\": \"Order ID and email match our records.\"\n",
    "    }\n",
    "\n",
    "# Test the tools\n",
    "print(\"\\n--- Testing FAQ Lookup ---\")\n",
    "faq_args = FAQLookupArgs(query=\"forgot password\", tags=[\"password\"])\n",
    "print(lookup_faq_answer(faq_args))\n",
    "\n",
    "print(\"\\n--- Testing Order Status Check ---\")\n",
    "order_args = CheckOrderStatusArgs(order_id=\"ABC-12345\", email=\"jane.smith@example.com\")\n",
    "print(json.dumps(check_order_status(order_args), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Tool Calling with LLM\n",
    "\n",
    "Let the LLM decide which tool to call based on the customer query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tool schemas\n",
    "TOOL_DEFINITIONS = [\n",
    "    {\n",
    "        \"name\": \"lookup_faq_answer\",\n",
    "        \"description\": \"Search the FAQ database for answers to common questions\",\n",
    "        \"parameters\": FAQLookupArgs.model_json_schema()\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"check_order_status\",\n",
    "        \"description\": \"Check the status of a customer order using order ID and email\",\n",
    "        \"parameters\": CheckOrderStatusArgs.model_json_schema()\n",
    "    }\n",
    "]\n",
    "\n",
    "def decide_next_action_with_tools(customer_query: CustomerQuery, max_retries: int = 3) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Use LLM to decide which tool to call based on customer query.\n",
    "    \"\"\"\n",
    "    tools_description = json.dumps(TOOL_DEFINITIONS, indent=2)\n",
    "    \n",
    "    prompt = f\"\"\"You are a customer support AI. Based on the customer query below, decide if you should call any tools.\n",
    "\n",
    "Available Tools:\n",
    "{tools_description}\n",
    "\n",
    "Customer Query:\n",
    "{customer_query.model_dump_json(indent=2)}\n",
    "\n",
    "Instructions:\n",
    "- If the query mentions an order_id, call check_order_status\n",
    "- If the query is about password, returns, or shipping, call lookup_faq_answer\n",
    "- Return ONLY a JSON object in this format:\n",
    "\n",
    "{{\n",
    "    \"should_call_tool\": true/false,\n",
    "    \"tool_name\": \"tool_name_here\" or null,\n",
    "    \"tool_arguments\": {{}} or null,\n",
    "    \"reasoning\": \"brief explanation\"\n",
    "}}\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nAttempt {attempt + 1} to decide tool calling...\")\n",
    "            response = call_hf_model(prompt, temperature=0.1)\n",
    "            \n",
    "            if not response:\n",
    "                continue\n",
    "            \n",
    "            json_str = extract_json_from_response(response)\n",
    "            tool_decision = json.loads(json_str)\n",
    "            \n",
    "            print(f\"✓ Tool decision: {tool_decision.get('reasoning', 'No reasoning provided')}\")\n",
    "            return tool_decision\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"✗ JSON decode error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test tool decision\n",
    "tool_decision = decide_next_action_with_tools(customer_query)\n",
    "if tool_decision:\n",
    "    print(f\"\\nTool Decision:\\n{json.dumps(tool_decision, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Execute Tool Calls with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tool_call(tool_decision: dict) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Execute the tool call with Pydantic validation of arguments.\n",
    "    \"\"\"\n",
    "    if not tool_decision.get(\"should_call_tool\"):\n",
    "        print(\"No tool call needed\")\n",
    "        return None\n",
    "    \n",
    "    tool_name = tool_decision.get(\"tool_name\")\n",
    "    tool_args = tool_decision.get(\"tool_arguments\")\n",
    "    \n",
    "    if not tool_name or not tool_args:\n",
    "        print(\"Invalid tool decision format\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if tool_name == \"lookup_faq_answer\":\n",
    "            # Validate arguments with Pydantic\n",
    "            validated_args = FAQLookupArgs.model_validate(tool_args)\n",
    "            print(f\"✓ Tool arguments validated for {tool_name}\")\n",
    "            result = lookup_faq_answer(validated_args)\n",
    "            return {\"tool_name\": tool_name, \"result\": result}\n",
    "            \n",
    "        elif tool_name == \"check_order_status\":\n",
    "            # Validate arguments with Pydantic\n",
    "            validated_args = CheckOrderStatusArgs.model_validate(tool_args)\n",
    "            print(f\"✓ Tool arguments validated for {tool_name}\")\n",
    "            result = check_order_status(validated_args)\n",
    "            return {\"tool_name\": tool_name, \"result\": result}\n",
    "            \n",
    "        else:\n",
    "            print(f\"Unknown tool: {tool_name}\")\n",
    "            return None\n",
    "            \n",
    "    except ValidationError as e:\n",
    "        print(f\"✗ Tool argument validation error:\")\n",
    "        for error in e.errors():\n",
    "            print(f\"  - {error['loc']}: {error['msg']}\")\n",
    "        return None\n",
    "\n",
    "# Execute the tool\n",
    "tool_output = execute_tool_call(tool_decision)\n",
    "if tool_output:\n",
    "    print(f\"\\nTool Output:\\n{json.dumps(tool_output, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Generate Final Support Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_support_ticket(\n",
    "    customer_query: CustomerQuery,\n",
    "    tool_output: Optional[dict] = None,\n",
    "    max_retries: int = 3\n",
    ") -> Optional[SupportTicket]:\n",
    "    \"\"\"\n",
    "    Generate final structured support ticket.\n",
    "    \"\"\"\n",
    "    schema = SupportTicket.model_json_schema()\n",
    "    \n",
    "    tool_info = \"No tools were called.\"\n",
    "    if tool_output:\n",
    "        tool_info = f\"Tool called: {tool_output['tool_name']}\\nResult: {json.dumps(tool_output['result'], indent=2)}\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a customer support AI. Create a final support ticket based on all information below.\n",
    "\n",
    "Support Ticket Schema:\n",
    "{json.dumps(schema, indent=2)}\n",
    "\n",
    "Customer Query:\n",
    "{customer_query.model_dump_json(indent=2)}\n",
    "\n",
    "Tool Information:\n",
    "{tool_info}\n",
    "\n",
    "Instructions:\n",
    "- Return ONLY valid JSON matching the SupportTicket schema\n",
    "- Include order_details if order information was retrieved\n",
    "- Include faq_response if FAQ was looked up\n",
    "- Choose appropriate recommended_next_action\n",
    "- Do NOT include creation_date (will be added automatically)\n",
    "\n",
    "JSON Response:\"\"\"\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"\\nAttempt {attempt + 1} to generate support ticket...\")\n",
    "            response = call_hf_model(prompt, temperature=0.2)\n",
    "            \n",
    "            if not response:\n",
    "                continue\n",
    "            \n",
    "            json_str = extract_json_from_response(response)\n",
    "            \n",
    "            # Parse and validate\n",
    "            ticket_data = json.loads(json_str)\n",
    "            \n",
    "            # Add creation date\n",
    "            ticket_data['creation_date'] = datetime.now().isoformat()\n",
    "            \n",
    "            # Validate with Pydantic\n",
    "            support_ticket = SupportTicket.model_validate(ticket_data)\n",
    "            print(\"✓ Support ticket generated and validated successfully\")\n",
    "            return support_ticket\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            print(f\"✗ Validation error on attempt {attempt + 1}:\")\n",
    "            for error in e.errors():\n",
    "                print(f\"  - {error['loc']}: {error['msg']}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                print(\"Max retries reached\")\n",
    "                return None\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"✗ JSON decode error: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Generate final ticket\n",
    "support_ticket = generate_support_ticket(customer_query, tool_output)\n",
    "if support_ticket:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL SUPPORT TICKET\")\n",
    "    print('='*60)\n",
    "    print(support_ticket.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Complete End-to-End Pipeline\n",
    "\n",
    "Now let's put everything together in a single function and test with multiple scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_customer_request(user_input_json: str) -> Optional[SupportTicket]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: validate input -> create query -> call tools -> generate ticket.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING CUSTOMER REQUEST\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Validate input\n",
    "    print(\"\\n[1/5] Validating user input...\")\n",
    "    validated_input = validate_user_input(user_input_json)\n",
    "    if not validated_input:\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Create customer query\n",
    "    print(\"\\n[2/5] Creating customer query...\")\n",
    "    customer_query = create_customer_query(validated_input)\n",
    "    if not customer_query:\n",
    "        return None\n",
    "    \n",
    "    # Step 3: Decide on tool calling\n",
    "    print(\"\\n[3/5] Deciding on tool calls...\")\n",
    "    tool_decision = decide_next_action_with_tools(customer_query)\n",
    "    \n",
    "    # Step 4: Execute tools if needed\n",
    "    tool_output = None\n",
    "    if tool_decision and tool_decision.get(\"should_call_tool\"):\n",
    "        print(\"\\n[4/5] Executing tool call...\")\n",
    "        tool_output = execute_tool_call(tool_decision)\n",
    "    else:\n",
    "        print(\"\\n[4/5] No tool call needed\")\n",
    "    \n",
    "    # Step 5: Generate final ticket\n",
    "    print(\"\\n[5/5] Generating support ticket...\")\n",
    "    support_ticket = generate_support_ticket(customer_query, tool_output)\n",
    "    \n",
    "    return support_ticket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: Order Status Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"TEST CASE 1: Order Status Query\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "test_input_1 = json.dumps({\n",
    "    \"name\": \"Jane Smith\",\n",
    "    \"email\": \"jane.smith@example.com\",\n",
    "    \"message\": \"What is the status of my order?\",\n",
    "    \"order_id\": \"ABC-12345\"\n",
    "})\n",
    "\n",
    "ticket_1 = process_customer_request(test_input_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Password Reset Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"TEST CASE 2: Password Reset Question\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "test_input_2 = json.dumps({\n",
    "    \"name\": \"Bob Jones\",\n",
    "    \"email\": \"bob.jones@example.com\",\n",
    "    \"message\": \"I forgot my password and can't log in. How do I reset it?\",\n",
    "    \"order_id\": None\n",
    "})\n",
    "\n",
    "ticket_2 = process_customer_request(test_input_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: Product Complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"TEST CASE 3: Product Complaint\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "test_input_3 = json.dumps({\n",
    "    \"name\": \"Joe User\",\n",
    "    \"email\": \"joe.user@example.com\",\n",
    "    \"message\": \"I'm really not happy with the product I bought. It doesn't work as advertised.\",\n",
    "    \"order_id\": \"XYZ-98765\"\n",
    "})\n",
    "\n",
    "ticket_3 = process_customer_request(test_input_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Test Invalid Input (Demonstrating Pydantic Validation)\n",
    "\n",
    "Let's see how Pydantic catches validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 4: Invalid Order ID Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"TEST CASE 4: Invalid Input (Wrong Order ID Format)\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "invalid_input = json.dumps({\n",
    "    \"name\": \"Test User\",\n",
    "    \"email\": \"test@example.com\",\n",
    "    \"message\": \"Check my order please\",\n",
    "    \"order_id\": \"12345\"  # Wrong format! Should be ABC-12345\n",
    "})\n",
    "\n",
    "ticket_invalid = process_customer_request(invalid_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 5: Invalid Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*60)\n",
    "print(\"TEST CASE 5: Invalid Email\")\n",
    "print(\"#\"*60)\n",
    "\n",
    "invalid_email = json.dumps({\n",
    "    \"name\": \"Test User\",\n",
    "    \"email\": \"not-an-email\",  # Invalid email format\n",
    "    \"message\": \"I need help with my account\",\n",
    "    \"order_id\": None\n",
    "})\n",
    "\n",
    "ticket_invalid_2 = process_customer_request(invalid_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: How Pydantic Works with Free HuggingFace Models\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "✓ Pydantic DOES work with free HuggingFace models!\n",
    "\n",
    "Key Points:\n",
    "1. DATA VALIDATION: Pydantic validates all inputs/outputs at every stage\n",
    "2. STRUCTURED OUTPUTS: We use JSON schemas to guide the LLM\n",
    "3. ERROR HANDLING: Retry logic handles when models produce invalid JSON\n",
    "4. TOOL CALLING: Pydantic validates tool arguments before execution\n",
    "5. COST: Completely free with HuggingFace Inference API\n",
    "\n",
    "Challenges with Smaller Models:\n",
    "- 7B models are less consistent than GPT-4/Claude at producing valid JSON\n",
    "- Require more retries and careful prompt engineering\n",
    "- May need temperature tuning (lower = more consistent)\n",
    "- JSON extraction from responses can be messier\n",
    "\n",
    "Best Practices:\n",
    "✓ Use clear, explicit prompts with schema examples\n",
    "✓ Implement retry logic with Pydantic validation\n",
    "✓ Extract JSON carefully from responses\n",
    "✓ Use lower temperature (0.1-0.3) for structured outputs\n",
    "✓ Validate at every step with Pydantic models\n",
    "\n",
    "Models that work well (free on HuggingFace):\n",
    "- mistralai/Mistral-7B-Instruct-v0.2 ✓\n",
    "- meta-llama/Meta-Llama-3-8B-Instruct ✓\n",
    "- microsoft/Phi-3-mini-4k-instruct ✓\n",
    "- HuggingFaceH4/zephyr-7b-beta ✓\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Model Comparison Function\n",
    "\n",
    "Test the same input across different models to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_performance(user_input_json: str, models: list[str]):\n",
    "    \"\"\"\n",
    "    Test the same input across different models to see which performs best.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"\\n\\nTesting with {model_name}...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        global MODEL_NAME\n",
    "        MODEL_NAME = model_name\n",
    "        \n",
    "        try:\n",
    "            ticket = process_customer_request(user_input_json)\n",
    "            results[model_name] = {\n",
    "                \"success\": ticket is not None,\n",
    "                \"ticket\": ticket.model_dump() if ticket else None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[model_name] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(\"COMPARISON RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    for model, result in results.items():\n",
    "        status = \"✓ SUCCESS\" if result[\"success\"] else \"✗ FAILED\"\n",
    "        print(f\"\\n{model}: {status}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Comparison (Optional - Takes Time)\n",
    "\n",
    "Uncomment and run to compare different models. This will take several minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test multiple models (this will take a while!)\n",
    "# test_models = [\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "# ]\n",
    "# \n",
    "# test_input = json.dumps({\n",
    "#     \"name\": \"Test User\",\n",
    "#     \"email\": \"test@example.com\",\n",
    "#     \"message\": \"What is the status of my order?\",\n",
    "#     \"order_id\": \"ABC-12345\"\n",
    "# })\n",
    "# \n",
    "# comparison = compare_model_performance(test_input, test_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrates that **Pydantic absolutely works with free HuggingFace models**, though with some caveats:\n",
    "\n",
    "1. **Validation is rock-solid** - Pydantic catches errors at every stage\n",
    "2. **Smaller models require more hand-holding** - More retries, better prompts, JSON extraction\n",
    "3. **The pattern is the same** - Whether using GPT-4, Claude, or Mistral-7B, the Pydantic workflow is identical\n",
    "4. **Cost-effective** - Everything in this tutorial can run on HuggingFace's free tier\n",
    "\n",
    "The key insight remains true: **It's Pydantic all the way down** in LLM workflows, regardless of which model you use!\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different HuggingFace models\n",
    "- Experiment with temperature settings\n",
    "- Add more tools to the system\n",
    "- Build your own custom validation rules\n",
    "- Deploy this to a real application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
