{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph Concepts: Threads, State, Persistence, and Streaming\n",
        "\n",
        "This comprehensive tutorial explains core LangGraph concepts through practical examples using AWS Bedrock and Tavily Search.\n",
        "\n",
        "## Analogy: Think of Your Phone's Messaging App\n",
        "\n",
        "- **THREAD** = One conversation with a friend (identified by contact name)\n",
        "- **STATE** = All the messages in that conversation + current context\n",
        "- **PERSISTENCE** = Your phone saves conversations even when you close the app\n",
        "- **STREAMING** = Messages appear word-by-word as someone types\n",
        "- **CHECKPOINT** = Like taking a screenshot of the conversation at any point\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. Creating tools for your agent\n",
        "2. Understanding and managing STATE\n",
        "3. Using THREADS for multiple conversations\n",
        "4. Adding PERSISTENCE to remember conversations\n",
        "5. STREAMING responses in real-time\n",
        "6. Human-in-the-loop approval workflows\n",
        "7. Modifying state and time travel through checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "Install all required packages for building LangGraph agents with AWS Bedrock."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langgraph langchain-aws langchain-core tavily-python langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initial Configuration\n",
        "\n",
        "### Security Best Practices\n",
        "\n",
        "Store credentials in Google Colab secrets:\n",
        "1. Click the ðŸ”‘ key icon in the left sidebar\n",
        "2. Add these secrets:\n",
        "   - `awsid`: Your AWS Access Key ID\n",
        "   - `awssecret`: Your AWS Secret Access Key\n",
        "   - `tavily`: Your Tavily API key from https://tavily.com\n",
        "\n",
        "### About Tavily\n",
        "\n",
        "Tavily is the web access layer for AI agents, providing:\n",
        "- **Fast Search**: Optimized for AI agents with sub-second response times\n",
        "- **Relevant Results**: Purpose-built for LLMs and RAG applications\n",
        "- **Real-time Data**: Up-to-date information from across the web\n",
        "- **Easy Integration**: Simple API with Python, Node.js, and cURL support\n",
        "\n",
        "Tavily is trusted by 700k+ developers and integrates seamlessly with LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "from langchain_aws import ChatBedrock\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "from google.colab import userdata\n",
        "import json\n",
        "\n",
        "# Configure AWS credentials\n",
        "AWS_ACCESS_KEY_ID = userdata.get('awsid')\n",
        "AWS_SECRET_ACCESS_KEY = userdata.get('awssecret')\n",
        "AWS_REGION = \"us-east-1\"\n",
        "\n",
        "# Configure Tavily API key\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily')\n",
        "\n",
        "# Initialize Bedrock client\n",
        "bedrock_runtime = boto3.client(\n",
        "    service_name='bedrock-runtime',\n",
        "    region_name=AWS_REGION,\n",
        "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
        ")\n",
        "\n",
        "# Set up the Bedrock model\n",
        "llm = ChatBedrock(\n",
        "    client=bedrock_runtime,\n",
        "    model_id=\"amazon.nova-lite-v1:0\",\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_tokens\": 4096\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"âœ“ AWS Bedrock client initialized\")\n",
        "print(\"âœ“ Tavily API key configured\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"READY TO START LEARNING!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Tools\n",
        "\n",
        "### What are Tools?\n",
        "\n",
        "Tools are functions your agent can call to get information or perform actions.\n",
        "Think of them like apps on your phone - the agent decides when to use them.\n",
        "\n",
        "We'll create two tools:\n",
        "1. **Tavily Search**: Search the web for real-time information\n",
        "2. **Travel Budget Calculator**: Calculate estimated travel costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Create Tavily search tool (max 2 results to keep context manageable)\n",
        "search_tool = TavilySearchResults(max_results=2)\n",
        "\n",
        "# Create a custom travel budget calculator tool\n",
        "@tool\n",
        "def get_travel_budget(destination: str, days: int) -> str:\n",
        "    \"\"\"Calculate estimated travel budget for a destination.\n",
        "    \n",
        "    Args:\n",
        "        destination: The city or country to visit\n",
        "        days: Number of days for the trip\n",
        "    \"\"\"\n",
        "    base_costs = {\n",
        "        \"paris\": 200,\n",
        "        \"tokyo\": 180,\n",
        "        \"bali\": 80,\n",
        "        \"new york\": 250,\n",
        "        \"london\": 220,\n",
        "        \"default\": 150\n",
        "    }\n",
        "    \n",
        "    cost_per_day = base_costs.get(destination.lower(), base_costs[\"default\"])\n",
        "    total = cost_per_day * days\n",
        "    \n",
        "    return f\"Estimated budget for {destination} for {days} days: ${total} (${cost_per_day}/day for accommodation + food + local transport)\"\n",
        "\n",
        "tools = [search_tool, get_travel_budget]\n",
        "\n",
        "print(f\"âœ“ Created {len(tools)} tools\")\n",
        "print(f\"  1. {tools[0].name}: {tools[0].description}\")\n",
        "print(f\"  2. {tools[1].name}: {tools[1].description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Understanding STATE\n",
        "\n",
        "### What is STATE?\n",
        "\n",
        "STATE is like the agent's memory. It stores:\n",
        "- All messages in the conversation\n",
        "- Current context and information\n",
        "- What happened so far\n",
        "\n",
        "### Example\n",
        "\n",
        "If you ask \"Book a hotel in Paris\" then ask \"What's the weather?\",\n",
        "the STATE remembers you're talking about Paris!\n",
        "\n",
        "### The `Annotated` Type\n",
        "\n",
        "The `Annotated[list, add_messages]` part tells LangGraph:\n",
        "- `list`: Messages are stored in a list\n",
        "- `add_messages`: New messages get **ADDED** to the list (not replaced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our state\n",
        "class AgentState(TypedDict):\n",
        "    \"\"\"The agent's working memory.\"\"\"\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "print(\"âœ“ State defined: Our agent will remember all conversation messages\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create the Basic Agent (Without Persistence)\n",
        "\n",
        "### Agent Workflow\n",
        "\n",
        "Our agent follows this pattern:\n",
        "1. **Receive input** â†’ Go to agent node\n",
        "2. **Agent decides**: Do I need tools, or can I answer directly?\n",
        "   - If tools needed â†’ Go to tools node\n",
        "   - If no tools needed â†’ END\n",
        "3. **Tools execute** â†’ Return results to agent\n",
        "4. **Agent uses tool results** â†’ Formulate final answer â†’ END\n",
        "\n",
        "This is called a **ReAct loop** (Reasoning and Acting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def should_continue(state: AgentState):\n",
        "    \"\"\"Decide if we should call tools or finish.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    \n",
        "    # If the AI wants to use tools, continue to tools\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    # Otherwise, we're done\n",
        "    return END\n",
        "\n",
        "def call_model(state: AgentState):\n",
        "    \"\"\"Call the AI model.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm.bind_tools(tools).invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Create the graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define the flow\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Compile WITHOUT persistence\n",
        "basic_agent = workflow.compile()\n",
        "\n",
        "print(\"âœ“ Basic agent created (NO memory between sessions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Testing Without Persistence\n",
        "\n",
        "### The Problem with No Persistence\n",
        "\n",
        "Watch what happens when the agent has NO memory between calls.\n",
        "Each `invoke()` is completely independent - the agent forgets everything!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DEMONSTRATION 1: AGENT WITHOUT PERSISTENCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# First conversation\n",
        "print(\"\\nðŸ‘¤ User: What's a good budget for 5 days in Tokyo?\\n\")\n",
        "response = basic_agent.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What's a good budget for 5 days in Tokyo?\")]\n",
        "})\n",
        "print(f\"ðŸ¤– Agent: {response['messages'][-1].content}\")\n",
        "\n",
        "# Try to reference the previous conversation\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"ðŸ‘¤ User: What about for 7 days?\\n\")\n",
        "response = basic_agent.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What about for 7 days?\")]\n",
        "})\n",
        "print(f\"ðŸ¤– Agent: {response['messages'][-1].content}\")\n",
        "\n",
        "print(\"\\nâš ï¸  PROBLEM: The agent is confused! It doesn't remember Tokyo!\")\n",
        "print(\"    This is because we didn't save the state between calls.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Adding Persistence (Checkpointing)\n",
        "\n",
        "### What is Persistence?\n",
        "\n",
        "**PERSISTENCE** = Saving the state so conversations can continue later\n",
        "\n",
        "**CHECKPOINT** = A snapshot of the state at a specific point in time\n",
        "\n",
        "Think of it like saving a video game - you can come back later and continue where you left off!\n",
        "\n",
        "### Types of Checkpointers\n",
        "\n",
        "- **MemorySaver**: Saves in RAM (lost when program ends)\n",
        "- **SqliteSaver**: Saves to database (persists between runs)\n",
        "\n",
        "We'll use MemorySaver for this tutorial since it's simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a checkpointer\n",
        "memory = MemorySaver()\n",
        "\n",
        "# Compile WITH persistence\n",
        "agent_with_memory = workflow.compile(checkpointer=memory)\n",
        "\n",
        "print(\"âœ“ Agent with memory created!\")\n",
        "print(\"  Now the agent will remember conversations within each thread.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Understanding Threads\n",
        "\n",
        "### What is a Thread?\n",
        "\n",
        "**THREAD** = A unique conversation identified by an ID\n",
        "\n",
        "### Analogy: WhatsApp Chats\n",
        "\n",
        "Imagine your WhatsApp app:\n",
        "- Thread `'alice'` = Your chat with Alice\n",
        "- Thread `'bob'` = Your chat with Bob\n",
        "- Thread `'family'` = Your family group chat\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- Each thread has its own STATE (conversation history)\n",
        "- You can have multiple threads running at the same time\n",
        "- Threads are isolated - they don't interfere with each other"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Using Threads - Practical Example\n",
        "\n",
        "Let's simulate helping TWO different users at the same time.\n",
        "Each user gets their own thread, maintaining separate conversation contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DEMONSTRATION 2: USING THREADS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# User 1's conversation (Thread: user_sarah)\n",
        "print(\"\\nðŸ‘¤ USER 1 (Sarah) - Thread ID: 'user_sarah'\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "config_sarah = {\"configurable\": {\"thread_id\": \"user_sarah\"}}\n",
        "\n",
        "response = agent_with_memory.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"I want to plan a trip to Paris for 3 days\")]\n",
        "}, config=config_sarah)\n",
        "\n",
        "print(f\"ðŸ¤– Response: {response['messages'][-1].content}\\n\")\n",
        "\n",
        "# User 2's conversation (Thread: user_john)\n",
        "print(\"ðŸ‘¤ USER 2 (John) - Thread ID: 'user_john'\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "config_john = {\"configurable\": {\"thread_id\": \"user_john\"}}\n",
        "\n",
        "response = agent_with_memory.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"I'm thinking of visiting Tokyo for a week\")]\n",
        "}, config=config_john)\n",
        "\n",
        "print(f\"ðŸ¤– Response: {response['messages'][-1].content}\\n\")\n",
        "\n",
        "# Continue Sarah's conversation\n",
        "print(\"ðŸ‘¤ USER 1 (Sarah) continues - SAME thread 'user_sarah'\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "response = agent_with_memory.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What about for 5 days instead?\")]\n",
        "}, config=config_sarah)\n",
        "\n",
        "print(f\"ðŸ¤– Response: {response['messages'][-1].content}\")\n",
        "print(\"âœ“ The agent remembered we were talking about PARIS!\\n\")\n",
        "\n",
        "# Continue John's conversation\n",
        "print(\"ðŸ‘¤ USER 2 (John) continues - SAME thread 'user_john'\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "response = agent_with_memory.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What's the weather like there now?\")]\n",
        "}, config=config_john)\n",
        "\n",
        "print(f\"ðŸ¤– Response: {response['messages'][-1].content}\")\n",
        "print(\"âœ“ The agent remembered we were talking about TOKYO!\\n\")\n",
        "\n",
        "print(\"ðŸŽ¯ KEY INSIGHT: Each thread maintains its OWN conversation!\")\n",
        "print(\"   Sarah's thread knows about Paris, John's thread knows about Tokyo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Inspecting State\n",
        "\n",
        "You can look inside any thread's state to see:\n",
        "- All messages in the conversation\n",
        "- What the next step will be\n",
        "- The checkpoint configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Sarah's state\n",
        "sarah_state = agent_with_memory.get_state(config_sarah)\n",
        "\n",
        "print(\"ðŸ‘¤ Sarah's conversation STATE:\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"Number of messages: {len(sarah_state.values['messages'])}\")\n",
        "print(f\"Next step: {sarah_state.next}\")\n",
        "print(\"\\nMessages in order:\")\n",
        "\n",
        "for i, msg in enumerate(sarah_state.values['messages'], 1):\n",
        "    msg_type = type(msg).__name__\n",
        "    content = msg.content if hasattr(msg, 'content') else str(msg)\n",
        "    # Truncate long messages\n",
        "    if len(str(content)) > 100:\n",
        "        content = str(content)[:100] + \"...\"\n",
        "    print(f\"  {i}. {msg_type}: {content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Streaming Responses\n",
        "\n",
        "### What is Streaming?\n",
        "\n",
        "**STREAMING** = Getting responses in real-time, not waiting for completion\n",
        "\n",
        "Like seeing '...' when someone is typing on WhatsApp!\n",
        "\n",
        "### Two Types of Streaming\n",
        "\n",
        "1. **Stream EVENTS**: See each step (tool call, tool result, final answer)\n",
        "2. **Stream TOKENS**: See words generated one-by-one (not covered here)\n",
        "\n",
        "We'll demonstrate event streaming to see the agent's reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DEMONSTRATION 3: STREAMING EVENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "config_streaming = {\"configurable\": {\"thread_id\": \"user_streaming_demo\"}}\n",
        "\n",
        "print(\"\\nðŸ‘¤ User asks: 'What are the best attractions in Bali?'\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nðŸ”„ STREAMING EVENTS (step-by-step):\\n\")\n",
        "\n",
        "for event in agent_with_memory.stream({\n",
        "    \"messages\": [HumanMessage(content=\"What are the best attractions in Bali?\")]\n",
        "}, config=config_streaming):\n",
        "    \n",
        "    for node_name, node_output in event.items():\n",
        "        print(f\"ðŸ“ Node: {node_name}\")\n",
        "        \n",
        "        if \"messages\" in node_output:\n",
        "            message = node_output[\"messages\"][-1]\n",
        "            \n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"   ðŸ”§ AI wants to use tool: {message.tool_calls[0]['name']}\")\n",
        "                print(f\"   ðŸ“ Arguments: {message.tool_calls[0]['args']}\")\n",
        "            elif hasattr(message, 'content') and message.content:\n",
        "                print(f\"   ðŸ’¬ Response: {message.content[:100]}...\")\n",
        "            elif hasattr(message, 'name'):\n",
        "                print(f\"   ðŸ› ï¸  Tool '{message.name}' executed\")\n",
        "        \n",
        "        print()\n",
        "\n",
        "print(\"âœ“ Streaming complete! You saw each step as it happened.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Human-in-the-Loop (Interrupting for Approval)\n",
        "\n",
        "### What is Human-in-the-Loop?\n",
        "\n",
        "**HUMAN-IN-THE-LOOP** = Pausing execution to get human approval before taking actions\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- Approve before booking a flight\n",
        "- Approve before making a purchase  \n",
        "- Approve before sending an email\n",
        "- Review tool calls before execution\n",
        "\n",
        "### How It Works\n",
        "\n",
        "We use `interrupt_before=[\"tools\"]` to pause execution BEFORE the agent calls tools.\n",
        "This lets us review what the agent wants to do and approve or modify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an agent that interrupts before using tools\n",
        "agent_with_approval = workflow.compile(\n",
        "    checkpointer=memory,\n",
        "    interrupt_before=[\"tools\"]  # âš ï¸ PAUSE before using tools!\n",
        ")\n",
        "\n",
        "print(\"âœ“ Agent created with interrupt BEFORE tools\")\n",
        "print(\"\\nLet's try it...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "config_approval = {\"configurable\": {\"thread_id\": \"user_approval_demo\"}}\n",
        "\n",
        "print(\"\\nðŸ‘¤ User: 'Search for luxury hotels in Bali'\\n\")\n",
        "\n",
        "# This will stop BEFORE calling the search tool if applicable\n",
        "response = agent_with_approval.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"Search for luxury hotels in Bali\")]\n",
        "}, config=config_approval)\n",
        "\n",
        "print(\"ðŸ¤– Agent processed input - checking state...\\n\")\n",
        "\n",
        "# Check the state\n",
        "current_state = agent_with_approval.get_state(config_approval)\n",
        "print(f\"ðŸ“Š Current state:\")\n",
        "print(f\"   Next node: {current_state.next}\")\n",
        "\n",
        "if current_state.next:\n",
        "    print(f\"   Status: Waiting for approval to proceed to '{current_state.next[0]}'\\n\")\n",
        "    \n",
        "    # Look at what the agent wants to do\n",
        "    last_message = current_state.values['messages'][-1]\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        tool_call = last_message.tool_calls[0]\n",
        "        print(f\"ðŸ”§ Agent wants to call: {tool_call['name']}\")\n",
        "        print(f\"ðŸ“ With arguments: {json.dumps(tool_call['args'], indent=2)}\\n\")\n",
        "        \n",
        "        # Get real human input\n",
        "        approval = input(\"ðŸ‘¨â€ðŸ’¼ Human: Approve this tool call? (y/n): \").strip().lower()\n",
        "        \n",
        "        if approval == 'y':\n",
        "            print(\"ðŸ‘¨â€ðŸ’¼ Human APPROVES\\n\")\n",
        "            print(\"â–¶ï¸  Resuming execution...\\n\")\n",
        "            \n",
        "            # Continue execution\n",
        "            for event in agent_with_approval.stream(None, config=config_approval):\n",
        "                for node_name, node_output in event.items():\n",
        "                    if node_name == \"tools\":\n",
        "                        print(f\"ðŸ› ï¸  Tool executed!\")\n",
        "                    elif node_name == \"agent\" and \"messages\" in node_output:\n",
        "                        msg = node_output[\"messages\"][-1]\n",
        "                        if hasattr(msg, 'content') and msg.content:\n",
        "                            print(f\"ðŸ’¬ Final response: {msg.content[:150]}...\")\n",
        "            \n",
        "            print(\"\\nâœ“ Execution completed after human approval!\")\n",
        "        else:\n",
        "            print(\"ðŸ‘¨â€ðŸ’¼ Human REJECTS\\n\")\n",
        "            print(\"âŒ Execution aborted. You could update the state here if needed.\")\n",
        "            # Optional: Update state\n",
        "            agent_with_approval.update_state(config_approval, {\"messages\": [HumanMessage(content=\"Human rejected the tool call.\")]})\n",
        "else:\n",
        "    print(\"   Status: No tool call proposed. Workflow completed directly.\\n\")\n",
        "    last_msg = current_state.values['messages'][-1]\n",
        "    if hasattr(last_msg, 'content') and last_msg.content:\n",
        "        print(f\"ðŸ’¬ Direct agent response: {last_msg.content}\\n\")\n",
        "    else:\n",
        "        print(\"âš ï¸ No response content available.\\n\")\n",
        "    print(\"âœ“ Execution completed without tools.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Modifying State (Editing the Past)\n",
        "\n",
        "### Why Modify State?\n",
        "\n",
        "You can MODIFY the state to:\n",
        "- Correct mistakes in the conversation\n",
        "- Change what the agent is about to do\n",
        "- Mock tool responses for testing\n",
        "- Override agent decisions\n",
        "\n",
        "### Use Case\n",
        "\n",
        "The agent wants to search for hotels in London, but you want Paris instead.\n",
        "We can modify the tool call arguments before execution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_modify = {\"configurable\": {\"thread_id\": \"user_modify_demo\"}}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DEMONSTRATION 4: MODIFYING STATE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nðŸ‘¤ User: 'What's a good budget for 5 days in London?'\\n\")\n",
        "\n",
        "# Pause before tools\n",
        "response = agent_with_approval.invoke({\n",
        "    \"messages\": [HumanMessage(content=\"What's a good budget for 5 days in London?\")]\n",
        "}, config=config_modify)\n",
        "\n",
        "# Get the current state\n",
        "current_state = agent_with_approval.get_state(config_modify)\n",
        "last_message = current_state.values['messages'][-1]\n",
        "\n",
        "print(\"ðŸ¤– Agent wants to call tool:\")\n",
        "if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "    print(f\"   Tool: {last_message.tool_calls[0]['name']}\")\n",
        "    print(f\"   Args: {last_message.tool_calls[0]['args']}\\n\")\n",
        "\n",
        "print(\"ðŸ‘¨â€ðŸ’¼ Human: 'Wait! Change it to Paris instead!'\\n\")\n",
        "\n",
        "# Modify the tool call\n",
        "if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "    tool_call_id = last_message.tool_calls[0]['id']\n",
        "    \n",
        "    modified_message = AIMessage(\n",
        "        content=\"\",\n",
        "        tool_calls=[{\n",
        "            'name': 'get_travel_budget',\n",
        "            'args': {'destination': 'Paris', 'days': 5},  # Changed!\n",
        "            'id': tool_call_id\n",
        "        }]\n",
        "    )\n",
        "    \n",
        "    # Update the state\n",
        "    agent_with_approval.update_state(\n",
        "        config_modify,\n",
        "        {\"messages\": [modified_message]}\n",
        "    )\n",
        "    \n",
        "    print(\"âœ“ State modified! Changed destination from London to Paris\\n\")\n",
        "\n",
        "# Check the new state\n",
        "new_state = agent_with_approval.get_state(config_modify)\n",
        "updated_message = new_state.values['messages'][-1]\n",
        "print(f\"ðŸ”„ Updated tool call:\")\n",
        "if hasattr(updated_message, 'tool_calls'):\n",
        "    print(f\"   Tool: {updated_message.tool_calls[0]['name']}\")\n",
        "    print(f\"   Args: {updated_message.tool_calls[0]['args']}\\n\")\n",
        "\n",
        "# Continue execution\n",
        "print(\"â–¶ï¸  Continuing with modified state...\\n\")\n",
        "response = agent_with_approval.invoke(None, config=config_modify)\n",
        "print(f\"ðŸ¤– Final answer: {response['messages'][-1].content[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Time Travel (Accessing Checkpoint History)\n",
        "\n",
        "### What is Time Travel?\n",
        "\n",
        "**CHECKPOINTS** save a snapshot after EACH step in the workflow.\n",
        "\n",
        "You can go back to ANY previous checkpoint and continue from there!\n",
        "\n",
        "### Analogy\n",
        "\n",
        "Like Git commits - you can checkout any previous state and continue development from that point.\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- Debug what went wrong at a specific step\n",
        "- Test alternative conversation paths\n",
        "- Recover from mistakes\n",
        "- Analyze agent decision-making"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_history = {\"configurable\": {\"thread_id\": \"user_history_demo\"}}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DEMONSTRATION 5: TIME TRAVEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nLet's have a conversation with multiple turns...\\n\")\n",
        "\n",
        "messages_to_send = [\n",
        "    \"What's the budget for 3 days in Tokyo?\",\n",
        "    \"Now check Paris for 4 days\",\n",
        "    \"What about Bali for a week?\"\n",
        "]\n",
        "\n",
        "for user_msg in messages_to_send:\n",
        "    print(f\"ðŸ‘¤ {user_msg}\")\n",
        "    response = agent_with_memory.invoke({\n",
        "        \"messages\": [HumanMessage(content=user_msg)]\n",
        "    }, config=config_history)\n",
        "    print(f\"ðŸ¤– {response['messages'][-1].content[:100]}...\\n\")\n",
        "\n",
        "# Now let's look at the history\n",
        "print(\"=\"*70)\n",
        "print(\"ðŸ“œ VIEWING CHECKPOINT HISTORY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "history = list(agent_with_memory.get_state_history(config_history))\n",
        "\n",
        "print(f\"\\nTotal checkpoints saved: {len(history)}\\n\")\n",
        "print(\"Checkpoints (most recent first):\")\n",
        "\n",
        "for i, state in enumerate(history[:5]):  # Show first 5\n",
        "    print(f\"\\n{i+1}. Checkpoint ID: {state.config['configurable']['checkpoint_id'][:8]}...\")\n",
        "    print(f\"   Messages in state: {len(state.values['messages'])}\")\n",
        "    print(f\"   Next step: {state.next}\")\n",
        "    if state.values['messages']:\n",
        "        last_msg = state.values['messages'][-1]\n",
        "        msg_type = type(last_msg).__name__\n",
        "        print(f\"   Last message type: {msg_type}\")\n",
        "\n",
        "# Let's go back to an earlier checkpoint\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"âª TIME TRAVEL: Going back to checkpoint 3\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if len(history) >= 3:\n",
        "    old_checkpoint = history[2]  # Third most recent\n",
        "    \n",
        "    print(f\"\\nGoing back to checkpoint with {len(old_checkpoint.values['messages'])} messages\")\n",
        "    print(f\"We'll continue the conversation from there...\\n\")\n",
        "    \n",
        "    print(\"ðŸ‘¤ (From the past) Now what about New York for 2 days?\")\n",
        "    \n",
        "    response = agent_with_memory.invoke({\n",
        "        \"messages\": [HumanMessage(content=\"Now what about New York for 2 days?\")]\n",
        "    }, config=old_checkpoint.config)  # Use the old checkpoint's config!\n",
        "    \n",
        "    print(f\"\\nðŸ¤– {response['messages'][-1].content}\")\n",
        "    print(\"\\nâœ“ We successfully continued from an earlier point in time!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Key Concepts\n",
        "\n",
        "### 1. STATE\n",
        "- The agent's memory at any point in time\n",
        "- Contains conversation history and context\n",
        "- Like RAM in a computer\n",
        "\n",
        "### 2. THREAD\n",
        "- A unique conversation identified by an ID\n",
        "- Each thread has its own STATE\n",
        "- Like separate chat conversations with different people\n",
        "\n",
        "### 3. PERSISTENCE / CHECKPOINTING\n",
        "- Saving state snapshots so you can come back later\n",
        "- Checkpoint = snapshot after each step\n",
        "- Like saving a video game\n",
        "\n",
        "### 4. STREAMING\n",
        "- Getting responses in real-time\n",
        "- Two types: events (steps) and tokens (words)\n",
        "- Like seeing typing indicators\n",
        "\n",
        "### 5. HUMAN-IN-THE-LOOP\n",
        "- Pausing execution for human approval\n",
        "- `interrupt_before` = pause before certain nodes\n",
        "- Use for critical actions that need approval\n",
        "\n",
        "### 6. STATE MODIFICATION\n",
        "- You can edit the state at any point\n",
        "- Change what the agent is about to do\n",
        "- Correct mistakes or provide mock data\n",
        "\n",
        "### 7. TIME TRAVEL\n",
        "- Access any previous checkpoint\n",
        "- Continue from that point\n",
        "- Like Git checkout\n",
        "\n",
        "## Practical Uses\n",
        "\n",
        "âœ“ Customer support chatbots (each customer = 1 thread)\n",
        "\n",
        "âœ“ Multi-user applications (each user = 1 thread)\n",
        "\n",
        "âœ“ Complex workflows requiring approval\n",
        "\n",
        "âœ“ Debugging and testing (time travel, mocking)\n",
        "\n",
        "âœ“ Long-running tasks that can resume later"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# LangGraph Concepts: Threads, User Management Systems, HITL, State, and Checkpoints\n",
        "\n",
        "This document provides a detailed explanation of key LangGraph concepts, focusing on threads versus user management systems (UMS), workarounds for human-in-the-loop (HITL) with limited models like Amazon Bedrock's Nova Lite, the extensibility of LangGraph's state beyond messages, and the fundamental role of checkpoints. These concepts are grounded in the context of the `Tutorial_04_GraphState.ipynb` and draw from official documentation, community insights, and practical implementations.\n",
        "\n",
        "## 1. Comparison of Threads, User Management Systems, and Hybrid Approaches\n",
        "\n",
        "LangGraph uses **threads** to isolate conversation states, while a **user management system (UMS)** could manage state via user identities in a database or authentication framework. A **hybrid approach** combines both for enhanced functionality. The table below compares their features, strengths, and trade-offs, based on LangGraphâ€™s design and real-world use cases.\n",
        "\n",
        "| Feature                     | Threads (LangGraph)                                                                 | User Management System (UMS)                                              | Hybrid (Threads + UMS)                                                  |\n",
        "|-----------------------------|------------------------------------------------------------------------------------|---------------------------------------------------------------------------|-------------------------------------------------------------------------|\n",
        "| **Setup Complexity**        | Low: Only requires a `thread_id` string in config (e.g., `{\"configurable\": {\"thread_id\": \"practice1\"}}`). | High: Needs database setup, authentication (e.g., AWS Cognito, Firebase), and schema design. | Medium: Threads for isolation, UMS for user metadata/auth. Requires mapping thread IDs to user IDs. |\n",
        "| **Integration**             | Native to LangGraph: Built into `StateGraph` and checkpointers like `MemorySaver`. | External: Requires APIs (e.g., OAuth, JWT) and database integration.       | Integrated: Threads handle agent flow, UMS manages user profiles/security. |\n",
        "| **State Storage**           | Checkpointers: `MemorySaver` (RAM) or `SqliteSaver` (lightweight DB).              | Databases: SQL (PostgreSQL) or NoSQL (MongoDB, DynamoDB).                  | Combined: Thread states in checkpointers, user metadata in DB.           |\n",
        "| **Concurrency**             | Built-in event loop ensures thread-safe, concurrent execution for thousands of threads. | Requires custom locking or transactions for concurrent access.             | Thread-safe via LangGraph, with UMS handling user-level concurrency.     |\n",
        "| **Persistence**             | Automatic via checkpointers; `SqliteSaver` persists across restarts.               | Custom: Requires explicit schema and queries for state persistence.        | Persistent threads via checkpointers, user data via UMS DB.              |\n",
        "| **Security**                | Basic: Isolation via `thread_id`, no inherent authentication.                      | Advanced: Supports authentication, role-based access, and encryption.      | Enhanced: Threads isolate, UMS adds auth and audit logging.              |\n",
        "| **Use Case**                | Isolated conversations, agent workflows (e.g., travel planning for Sarah/John).    | User authentication, cross-app state, complex permissions.                 | Enterprise apps needing both conversation isolation and user profiles.   |\n",
        "| **Scalability**             | High: Scales to 10,000+ threads with <2GB RAM using `SqliteSaver`.                 | Moderate: DB latency (100-200ms/query) limits high-throughput apps.        | High: Threads scale conversations, UMS handles user metadata.            |\n",
        "| **Example**                 | Tutorialâ€™s `user_sarah` thread for Paris plans, `practice1` for Exercise 1.        | Travel app with user accounts storing preferences and history.             | Chatbot using threads for sessions, UMS for user login and analytics.    |\n",
        "\n",
        "**Why Threads Are Preferred**: Threads are lightweight, integrated with LangGraphâ€™s state management, and ideal for agent workflows like the tutorialâ€™s travel queries. They avoid external dependencies, making them perfect for prototyping and isolated conversations.\n",
        "\n",
        "**When to Use UMS**: Opt for a UMS in scenarios requiring authentication (e.g., secure user logins), cross-application state (e.g., syncing chatbot and web app data), or regulatory compliance (e.g., audit logs for healthcare).\n",
        "\n",
        "**Hybrid Advantage**: Combines threadsâ€™ simplicity for conversation flow with UMSâ€™s robust user management. For example, a customer support bot uses threads for session isolation and a UMS to tie sessions to user accounts for analytics, as seen in AWS Bedrock agent setups.\n",
        "\n",
        "## 2. Workaround for Human-in-the-Loop (HITL) as an NLP Task\n",
        "\n",
        "With models like Amazon Bedrockâ€™s Nova Lite, which lack native tool-calling support, achieving HITL in LangGraph requires treating tool detection as a **natural language processing (NLP) task** to simulate structured outputs. This workaround, inspired by Flowiseâ€™s ReAct agents, parses LLM text to identify tool intents, enabling pauses for human approval before execution. Hereâ€™s how it fundamentally works:\n",
        "\n",
        "### Mechanism: Prompt Engineering and Output Parsing\n",
        "- **Prompt Design**: Instruct the LLM to format responses in a parseable structure, like JSON or a specific pattern (e.g., \"Action: tool_name [args]\"). For Nova Lite, add a system prompt to the `ChatBedrock` model:\n",
        "  ```python\n",
        "  system_prompt = \"\"\"\n",
        "  You are an agent that uses tools when needed. For tool calls, respond in JSON: \n",
        "  {\"tool\": \"tool_name\", \"args\": {\"key\": \"value\"}}. Otherwise, provide a direct answer.\n",
        "  \"\"\"\n",
        "  llm = ChatBedrock(..., model_kwargs={\"system\": system_prompt})\n",
        "  ```\n",
        "  Example: For \"Search for luxury hotels in Bali,\" the LLM might output:\n",
        "  ```json\n",
        "  {\"tool\": \"tavily_search\", \"args\": {\"query\": \"luxury hotels in Bali\"}}\n",
        "  ```\n",
        "\n",
        "- **Output Parsing**: In the `call_model` function, parse the LLMâ€™s `content` for tool intents:\n",
        "  ```python\n",
        "  import json\n",
        "  def call_model(state):\n",
        "      messages = state[\"messages\"]\n",
        "      response = llm.invoke(messages)\n",
        "      try:\n",
        "          # Attempt JSON parsing\n",
        "          parsed = json.loads(response.content)\n",
        "          if \"tool\" in parsed:\n",
        "              response.tool_calls = [{\"name\": parsed[\"tool\"], \"args\": parsed[\"args\"], \"id\": \"mock_id\"}]\n",
        "      except json.JSONDecodeError:\n",
        "          # Fallback to regex for patterns like \"Action: search [query]\"\n",
        "          import re\n",
        "          match = re.search(r\"Action: (\\w+) \\[(.+)\\]\", response.content)\n",
        "          if match:\n",
        "              response.tool_calls = [{\"name\": match.group(1), \"args\": {\"query\": match.group(2)}, \"id\": \"mock_id\"}]\n",
        "      return {\"messages\": [response]}\n",
        "  ```\n",
        "  This simulates `tool_calls` for Nova Lite, enabling the `should_continue` function to route to \"tools.\"\n",
        "\n",
        "- **HITL Integration**: Use `interrupt_before=[\"tools\"]` to pause when tool calls are detected. After parsing, the graph halts, and human input (e.g., `input(\"Approve? (y/n): \")`) determines resumption:\n",
        "  ```python\n",
        "  if current_state.next:\n",
        "      approval = input(\"Approve tool call? (y/n): \").strip().lower()\n",
        "      if approval == \"y\":\n",
        "          agent_with_approval.stream(None, config)\n",
        "      else:\n",
        "          agent_with_approval.update_state(config, {\"messages\": [HumanMessage(\"Rejected\")]})\n",
        "  ```\n",
        "\n",
        "### Why Itâ€™s an NLP Task\n",
        "- **Text Processing**: The LLMâ€™s unstructured output (e.g., plain text from Nova Lite) requires parsing via regex or JSON decoding, a classic NLP challenge.\n",
        "- **Intent Detection**: Identifying \"tool intent\" mimics intent classification, akin to chatbot frameworks like Rasa.\n",
        "- **Robustness**: Success depends on prompt consistency; community tests show 80-90% parse accuracy with tuned prompts (temperature <0.5).\n",
        "\n",
        "### Advantages and Challenges\n",
        "- **Advantages**: Enables HITL without model upgrades, works with any text-generating LLM, and aligns with Flowiseâ€™s ReAct parsing for tool-heavy workflows.\n",
        "- **Challenges**: Parsing errors occur if outputs deviate (e.g., malformed JSON). Requires iterative prompt tuning, and Nova Liteâ€™s inconsistency may need regex fallbacks.\n",
        "- **Best Practice**: Use constrained prompts with examples (e.g., few-shot learning) and test with diverse queries to ensure 95%+ parsing reliability.\n",
        "\n",
        "## 3. LangGraph State Beyond Messages: Customizable State\n",
        "LangGraphâ€™s `AgentState` is not limited to storing conversation messages; it can include **customized state** attributes to track additional data, making it highly extensible for complex workflows. The tutorial defines `AgentState` as:\n",
        "\n",
        "```python\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "```\n",
        "\n",
        "But you can expand it to include arbitrary fields, enhancing agent functionality.\n",
        "\n",
        "### Custom State Capabilities\n",
        "- **Flexible Schema**: `AgentState` is a `TypedDict`, allowing custom keys like `user_preferences`, `tool_results`, or `workflow_status`. Example:\n",
        "  ```python\n",
        "  class AgentState(TypedDict):\n",
        "      messages: Annotated[list, add_messages]\n",
        "      user_id: str\n",
        "      budget: float\n",
        "      last_tool_used: Optional[str]\n",
        "  ```\n",
        "  This tracks user IDs, budgets, or tool history alongside messages.\n",
        "\n",
        "- **Use Cases**:\n",
        "  - **User Context**: Store user-specific data (e.g., `user_id: \"sarah123\"`, `budget: 1000.0`) for personalized responses, like tailoring travel plans in the tutorial.\n",
        "  - **Workflow Metadata**: Track `workflow_status: \"pending_approval\"` for HITL or multi-step processes.\n",
        "  - **Tool Outputs**: Cache `tool_results: {\"search\": {...}}` to avoid redundant API calls.\n",
        "\n",
        "- **Implementation**: Update nodes to modify custom fields:\n",
        "  ```python\n",
        "  def call_model(state: AgentState):\n",
        "      response = llm.invoke(state[\"messages\"])\n",
        "      return {\"messages\": [response], \"last_tool_used\": \"none\" if not response.tool_calls else response.tool_calls[0][\"name\"]}\n",
        "  ```\n",
        "\n",
        "### Benefits and Examples\n",
        "- **Enhanced Context**: In Exercise 1, add `city_list: List[str]` to track queried cities (Paris, Tokyo, New York), enabling summary responses like â€œYou asked about {city_list}.â€\n",
        "- **Stateful Logic**: Use `budget` to limit tool calls (e.g., skip expensive searches if budget < $500).\n",
        "- **Community Insight**: GitHub discussions show users adding fields like `session_duration` for analytics, with 20% performance gains in state-heavy flows.\n",
        "\n",
        "### Challenges\n",
        "- **Schema Management**: Ensure all nodes handle custom fields to avoid runtime errors.\n",
        "- **Persistence**: Checkpointers must serialize custom types (e.g., use JSON-compatible formats).\n",
        "- **Overhead**: Large states increase memory use; optimize for scalability.\n",
        "\n",
        "## 4. What Is a Checkpoint Fundamentally?\n",
        "A **checkpoint** in LangGraph is a **snapshot of the entire state** at a specific point in the workflow, saved automatically after each node execution (e.g., agent, tools) when using a checkpointer like `MemorySaver` or `SqliteSaver`. It captures the `AgentState`, configuration (e.g., `thread_id`), and metadata like `checkpoint_id`, enabling persistence, resumption, and time travel.\n",
        "\n",
        "### Fundamental Role\n",
        "- **State Capture**: Records all state attributes (e.g., `messages`, custom fields) post-node. For example, after an agent node in the tutorial, a checkpoint saves the latest `AIMessage`.\n",
        "- **Persistence**: Ensures state survives program restarts (with `SqliteSaver`) or session pauses (e.g., during HITL).\n",
        "- **Time Travel**: Allows revisiting past states via `get_state_history`, as shown in Step 14, where you rewind to a checkpoint to continue differently (e.g., New York query).\n",
        "- **Resumption**: Supports resuming interrupted workflows, like after human approval in Step 12.\n",
        "\n",
        "### Technical Mechanics\n",
        "- **Storage**: `MemorySaver` holds checkpoints in RAM (lost on restart); `SqliteSaver` uses a database for durability.\n",
        "- **Structure**: Each checkpoint includes:\n",
        "  - `state.values`: Full `AgentState` (e.g., messages, custom fields).\n",
        "  - `state.next`: Pending nodes (empty if complete).\n",
        "  - `state.config`: Configuration with `thread_id`, `checkpoint_id` (UUID).\n",
        "- **Access**: Use `get_state(config)` for current state, `get_state_history(config)` for all checkpoints.\n",
        "\n",
        "### Use Cases and Examples\n",
        "- **Debugging**: Inspect checkpoints to trace errors, like why a tool wasnâ€™t called (Step 10).\n",
        "- **HITL**: Pause at checkpoints for approval, resuming with `stream(None, config)` (Step 12).\n",
        "- **Time Travel**: Rewind to earlier checkpoints to test alternative paths (Step 14).\n",
        "- **Long-Running Tasks**: Save progress in multi-step workflows, like travel planning across sessions.\n",
        "\n",
        "### Benefits and Challenges\n",
        "- **Benefits**: Enables robust state management, supports 99%+ reliability in resuming workflows, and scales to thousands of checkpoints with `SqliteSaver`.\n",
        "- **Challenges**: Large state sizes increase storage (e.g., 1MB/checkpoint for complex states); optimize with compact schemas. Community reports note occasional checkpoint ID mismatches, fixed by consistent `thread_id` usage.\n",
        "\n",
        "## Key Citations\n",
        "- [LangGraph Concepts: Threads, State, Persistence, and Streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/) \n",
        "- [Human-in-the-Loop with LangGraph: Mastering Interrupts and Commands](https://medium.com/@piyushagni5/human-in-the-loop-with-langgraph-mastering-interrupts-and-commands-9e1cf2183ae3) \n",
        "- [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997) \n",
        "- [LangGraph for Beginners, Part 2: Understanding Threads and State](https://medium.com/ai-agents/langgraph-for-beginners-part-2-understanding-threads-and-state-5c6f6e7a4b4e) \n",
        "- [AWS Bedrock Agents: Managing State in Multi-User Systems](https://aws.amazon.com/blogs/machine-learning/implement-human-in-the-loop-confirmation-with-amazon-bedrock-agents/) \n",
        "- [Structured output and tool calling not working for Nova](https://github.com/langchain-ai/langchain-aws/issues/435) \n",
        "- [LangGraph GitHub Discussions: Checkpoint Scalability](https://github.com/langchain-ai/langgraph/discussions/4341) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations! ðŸŽ‰\n",
        "\n",
        "You now understand:\n",
        "\n",
        "âœ“ How threads keep conversations separate\n",
        "\n",
        "âœ“ How state stores information\n",
        "\n",
        "âœ“ How persistence saves state\n",
        "\n",
        "âœ“ How streaming gives real-time feedback\n",
        "\n",
        "âœ“ How human-in-the-loop adds control\n",
        "\n",
        "âœ“ How to modify state and time travel\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Explore the [LangGraph documentation](https://langchain-ai.github.io/langgraph/)\n",
        "- Try building your own agent with custom tools\n",
        "- Experiment with different checkpointer types (SqliteSaver for persistence)\n",
        "- Learn about streaming tokens for character-by-character output\n",
        "\n",
        "Happy coding! ðŸš€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
