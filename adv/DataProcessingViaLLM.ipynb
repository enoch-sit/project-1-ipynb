{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Q&A System with AWS Bedrock\n",
    "\n",
    "This notebook demonstrates how to build a question-answering system over documents using:\n",
    "- **AWS Bedrock** (Amazon Nova Lite) for LLM capabilities\n",
    "- **Wikipedia API** to fetch current events data\n",
    "- **Embeddings and Vector Search** for document retrieval\n",
    "\n",
    "Based on concepts from LangChain's document Q&A approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 wikipedia-api numpy scikit-learn requests beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure AWS Bedrock\n",
    "\n",
    "Set up your AWS credentials and initialize the Bedrock client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Google Colab userdata for secure credential access\n",
    "from google.colab import userdata\n",
    "\n",
    "# Configure your AWS credentials using Colab secrets\n",
    "AWS_ACCESS_KEY_ID = userdata.get('awsid')  # Set this in Colab secrets\n",
    "AWS_SECRET_ACCESS_KEY = userdata.get('awssecret')  # Set this in Colab secrets\n",
    "AWS_REGION = \"us-east-1\"  # Change if needed\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "\n",
    "print(\"✓ AWS Bedrock client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Current Events from Wikipedia\n",
    "\n",
    "We'll scrape Wikipedia's Current Events portal to get recent news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_current_events(max_days=3):\n",
    "    \"\"\"\n",
    "    Fetch current events from Wikipedia's Portal:Current_events\n",
    "    \n",
    "    Args:\n",
    "        max_days: Number of recent days to fetch (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with date and events\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/Portal:Current_events\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        events_by_date = []\n",
    "        \n",
    "        # Find all date headers (h3 tags with date class)\n",
    "        date_sections = soup.find_all('h3')\n",
    "        \n",
    "        for i, date_section in enumerate(date_sections[:max_days]):\n",
    "            date_text = date_section.get_text().strip()\n",
    "            \n",
    "            # Find the content div following this header\n",
    "            content_div = date_section.find_next_sibling('div')\n",
    "            \n",
    "            if content_div:\n",
    "                # Extract all list items (events)\n",
    "                events = []\n",
    "                \n",
    "                # Find all categories and their events\n",
    "                categories = content_div.find_all('dl')\n",
    "                \n",
    "                for category in categories:\n",
    "                    # Get category name\n",
    "                    category_name = category.find('dt')\n",
    "                    if category_name:\n",
    "                        category_text = category_name.get_text().strip()\n",
    "                        \n",
    "                        # Get all events in this category\n",
    "                        event_items = category.find_all('dd')\n",
    "                        \n",
    "                        for event in event_items:\n",
    "                            event_text = event.get_text().strip()\n",
    "                            # Clean up the text\n",
    "                            event_text = re.sub(r'\\s+', ' ', event_text)\n",
    "                            \n",
    "                            if event_text:\n",
    "                                events.append({\n",
    "                                    'category': category_text,\n",
    "                                    'event': event_text\n",
    "                                })\n",
    "                \n",
    "                if events:\n",
    "                    events_by_date.append({\n",
    "                        'date': date_text,\n",
    "                        'events': events\n",
    "                    })\n",
    "        \n",
    "        return events_by_date\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching Wikipedia events: {e}\")\n",
    "        return []\n",
    "\n",
    "# Fetch current events\n",
    "print(\"Fetching current events from Wikipedia...\")\n",
    "events_data = fetch_wikipedia_current_events(max_days=3)\n",
    "\n",
    "print(f\"\\n✓ Fetched events from {len(events_data)} days\")\n",
    "for day_data in events_data:\n",
    "    print(f\"  - {day_data['date']}: {len(day_data['events'])} events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Fetched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample events\n",
    "if events_data:\n",
    "    print(\"\\n=== Sample Events ===\")\n",
    "    first_day = events_data[0]\n",
    "    print(f\"\\nDate: {first_day['date']}\")\n",
    "    print(\"\\nFirst 5 events:\")\n",
    "    \n",
    "    for i, event in enumerate(first_day['events'][:5], 1):\n",
    "        print(f\"\\n{i}. Category: {event['category']}\")\n",
    "        print(f\"   Event: {event['event'][:200]}...\" if len(event['event']) > 200 else f\"   Event: {event['event']}\")\n",
    "else:\n",
    "    print(\"No events fetched. Please check your internet connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Document Chunks\n",
    "\n",
    "Convert the events into document chunks for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_chunks(events_data):\n",
    "    \"\"\"\n",
    "    Create document chunks from events data.\n",
    "    Each chunk contains context about date, category, and event.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for day_data in events_data:\n",
    "        date = day_data['date']\n",
    "        \n",
    "        for event in day_data['events']:\n",
    "            # Create a comprehensive chunk with context\n",
    "            chunk_text = f\"\"\"Date: {date}\n",
    "Category: {event['category']}\n",
    "Event: {event['event']}\"\"\"\n",
    "            \n",
    "            chunks.append({\n",
    "                'text': chunk_text,\n",
    "                'date': date,\n",
    "                'category': event['category'],\n",
    "                'event': event['event']\n",
    "            })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create document chunks\n",
    "document_chunks = create_document_chunks(events_data)\n",
    "\n",
    "print(f\"\\n✓ Created {len(document_chunks)} document chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(\"=\"*50)\n",
    "print(document_chunks[0]['text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock Helper Functions\n",
    "\n",
    "Functions to interact with Amazon Nova Lite via Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_bedrock(text: str) -> List[float]:\n",
    "    \"\"\"\n",
    "    Get embeddings using Amazon Titan Embeddings via Bedrock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": text\n",
    "        })\n",
    "        \n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='amazon.titan-embed-text-v1',\n",
    "            body=body,\n",
    "            contentType='application/json',\n",
    "            accept='application/json'\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embedding']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        # Return a zero vector as fallback\n",
    "        return [0.0] * 1536\n",
    "\n",
    "\n",
    "def invoke_nova(prompt: str, system_prompt: str = \"\", max_tokens: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Invoke Amazon Nova Lite via AWS Bedrock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Combine system and user prompts for Nova Lite\n",
    "        if system_prompt:\n",
    "            full_prompt = f\"{system_prompt}\\n\\nHuman: {prompt}\\n\\nAssistant:\"\n",
    "        else:\n",
    "            full_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        body = {\n",
    "            \"inputText\": full_prompt,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": max_tokens,\n",
    "                \"temperature\": 0.7,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='amazon.nova-lite-v1:0',\n",
    "            body=json.dumps(body),\n",
    "            contentType='application/json',\n",
    "            accept='application/json'\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['results'][0]['outputText']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking Nova Lite: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "Create embeddings for all document chunks and build an in-memory vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple in-memory vector store using cosine similarity.\n",
    "    Similar to LangChain's DocArrayInMemorySearch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_documents(self, chunks: List[Dict]):\n",
    "        \"\"\"\n",
    "        Add documents and create embeddings.\n",
    "        \"\"\"\n",
    "        print(f\"Creating embeddings for {len(chunks)} documents...\")\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            \n",
    "            embedding = get_embedding_bedrock(chunk['text'])\n",
    "            self.documents.append(chunk)\n",
    "            self.embeddings.append(embedding)\n",
    "        \n",
    "        self.embeddings = np.array(self.embeddings)\n",
    "        print(f\"✓ Created embeddings with shape: {self.embeddings.shape}\")\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = 4) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find the k most similar documents to the query.\n",
    "        \"\"\"\n",
    "        # Get query embedding\n",
    "        query_embedding = np.array(get_embedding_bedrock(query)).reshape(1, -1)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        # Return top k documents with scores\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            doc = self.documents[idx].copy()\n",
    "            doc['similarity_score'] = float(similarities[idx])\n",
    "            results.append(doc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create and populate the vector store\n",
    "print(\"\\nInitializing vector store...\")\n",
    "vector_store = SimpleVectorStore()\n",
    "vector_store.add_documents(document_chunks)\n",
    "print(\"\\n✓ Vector store ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Document Retrieval\n",
    "\n",
    "Let's test the similarity search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with a sample query\n",
    "test_query = \"What happened in sports recently?\"\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(test_query, k=3)\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. Similarity Score: {doc['similarity_score']:.4f}\")\n",
    "    print(f\"   Date: {doc['date']}\")\n",
    "    print(f\"   Category: {doc['category']}\")\n",
    "    print(f\"   Event: {doc['event'][:150]}...\" if len(doc['event']) > 150 else f\"   Event: {doc['event']}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Q&A Chain\n",
    "\n",
    "Create the complete question-answering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentQAChain:\n",
    "    \"\"\"\n",
    "    A question-answering chain over documents.\n",
    "    Similar to LangChain's RetrievalQA chain.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: SimpleVectorStore, verbose: bool = False):\n",
    "        self.vector_store = vector_store\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def run(self, query: str, k: int = 4) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the QA chain on a query.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"QUERY: {query}\")\n",
    "            print(f\"{'='*70}\")\n",
    "        \n",
    "        # Step 1: Retrieve relevant documents\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[RETRIEVAL] Searching for {k} most relevant documents...\")\n",
    "        \n",
    "        retrieved_docs = self.vector_store.similarity_search(query, k=k)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n[RETRIEVED DOCUMENTS]\")\n",
    "            for i, doc in enumerate(retrieved_docs, 1):\n",
    "                print(f\"\\n  Document {i} (Score: {doc['similarity_score']:.4f}):\")\n",
    "                print(f\"  {doc['text'][:200]}...\" if len(doc['text']) > 200 else f\"  {doc['text']}\")\n",
    "        \n",
    "        # Step 2: Combine documents into context\n",
    "        context = \"\\n\\n\".join([doc['text'] for doc in retrieved_docs])\n",
    "        \n",
    "        # Step 3: Create prompt for Nova Lite\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "Use only the information from the context to answer the question. \n",
    "If you cannot answer based on the context, say so.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n[PROMPT TO LLM]\")\n",
    "            print(f\"System: {system_prompt}\")\n",
    "            print(f\"\\nUser: {user_prompt[:500]}...\" if len(user_prompt) > 500 else f\"\\nUser: {user_prompt}\")\n",
    "        \n",
    "        # Step 4: Get answer from Nova Lite\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[INVOKING NOVA LITE]...\")\n",
    "        \n",
    "        answer = invoke_nova(user_prompt, system_prompt=system_prompt)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n[RESPONSE]\")\n",
    "            print(f\"{answer}\")\n",
    "            print(f\"\\n{'='*70}\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'answer': answer,\n",
    "            'source_documents': retrieved_docs\n",
    "        }\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = DocumentQAChain(vector_store, verbose=True)\n",
    "\n",
    "print(\"\\n✓ QA Chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Question Answering\n",
    "\n",
    "Now let's ask questions about the current events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Sports question\n",
    "result = qa_chain.run(\"What sports events happened recently?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the answer nicely\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(f\"**Question:** {result['query']}\"))\n",
    "display(Markdown(f\"**Answer:** {result['answer']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Politics question\n",
    "result = qa_chain.run(\"Tell me about recent political events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**Question:** {result['query']}\"))\n",
    "display(Markdown(f\"**Answer:** {result['answer']}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Disaster/accident question\n",
    "result = qa_chain.run(\"Were there any disasters or accidents reported?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**Question:** {result['query']}\"))\n",
    "display(Markdown(f\"**Answer:** {result['answer']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Q&A\n",
    "\n",
    "Try your own questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off verbose mode for cleaner output\n",
    "qa_chain.verbose = False\n",
    "\n",
    "def ask_question(question: str):\n",
    "    \"\"\"\n",
    "    Helper function to ask questions with nice formatting.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    result = qa_chain.run(question)\n",
    "    \n",
    "    print(f\"\\nA: {result['answer']}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for i, doc in enumerate(result['source_documents'][:2], 1):\n",
    "        print(f\"  {i}. {doc['date']} - {doc['category']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try some questions\n",
    "ask_question(\"What happened in the Gaza war?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"Tell me about business and economy news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question(\"What international relations events occurred?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your custom question\n",
    "your_question = \"What elections or political changes happened recently?\"\n",
    "ask_question(your_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Generate Test Examples\n",
    "\n",
    "Let's use Nova Lite to automatically generate question-answer pairs for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_examples(documents: List[Dict], num_examples: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate question-answer pairs from documents using Nova Lite.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Sample documents\n",
    "    sampled_docs = np.random.choice(documents, min(num_examples, len(documents)), replace=False)\n",
    "    \n",
    "    print(f\"Generating {num_examples} Q&A examples...\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(sampled_docs, 1):\n",
    "        print(f\"Generating example {i}/{len(sampled_docs)}...\")\n",
    "        \n",
    "        prompt = f\"\"\"Based on the following event, create a question and answer pair.\n",
    "\n",
    "Event:\n",
    "{doc['text']}\n",
    "\n",
    "Generate:\n",
    "1. A specific question that can be answered from this event\n",
    "2. A concise answer to that question\n",
    "\n",
    "Format your response as:\n",
    "QUESTION: [your question]\n",
    "ANSWER: [your answer]\"\"\"\n",
    "        \n",
    "        response = invoke_nova(prompt)\n",
    "        \n",
    "        # Parse the response\n",
    "        try:\n",
    "            question_match = re.search(r'QUESTION:\\s*(.+?)(?=ANSWER:|$)', response, re.DOTALL)\n",
    "            answer_match = re.search(r'ANSWER:\\s*(.+)', response, re.DOTALL)\n",
    "            \n",
    "            if question_match and answer_match:\n",
    "                question = question_match.group(1).strip()\n",
    "                answer = answer_match.group(1).strip()\n",
    "                \n",
    "                examples.append({\n",
    "                    'question': question,\n",
    "                    'ground_truth_answer': answer,\n",
    "                    'source_document': doc\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"  Error parsing response: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(examples)} examples\")\n",
    "    return examples\n",
    "\n",
    "# Generate test examples\n",
    "test_examples = generate_qa_examples(document_chunks, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated examples\n",
    "print(\"\\n=== Generated Test Examples ===\")\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Q: {example['question']}\")\n",
    "    print(f\"A: {example['ground_truth_answer']}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Test the QA system on generated examples and evaluate using Nova Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_system(qa_chain: DocumentQAChain, test_examples: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Evaluate the QA system on test examples.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating QA system on {len(test_examples)} examples...\\n\")\n",
    "    \n",
    "    for i, example in enumerate(test_examples, 1):\n",
    "        print(f\"Testing example {i}/{len(test_examples)}...\")\n",
    "        \n",
    "        # Get prediction from QA chain\n",
    "        prediction = qa_chain.run(example['question'])\n",
    "        \n",
    "        # Evaluate with Nova Lite\n",
    "        eval_prompt = f\"\"\"Compare the following predicted answer with the ground truth answer.\n",
    "\n",
    "Question: {example['question']}\n",
    "\n",
    "Ground Truth Answer: {example['ground_truth_answer']}\n",
    "\n",
    "Predicted Answer: {prediction['answer']}\n",
    "\n",
    "Does the predicted answer correctly answer the question based on the ground truth?\n",
    "Respond with only: CORRECT or INCORRECT\n",
    "Then provide a brief explanation.\"\"\"\n",
    "        \n",
    "        evaluation = invoke_nova(eval_prompt)\n",
    "        \n",
    "        grade = \"CORRECT\" if \"CORRECT\" in evaluation.split()[0].upper() else \"INCORRECT\"\n",
    "        \n",
    "        results.append({\n",
    "            'question': example['question'],\n",
    "            'ground_truth': example['ground_truth_answer'],\n",
    "            'prediction': prediction['answer'],\n",
    "            'grade': grade,\n",
    "            'evaluation': evaluation\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation complete\")\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_qa_system(qa_chain, test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "correct_count = sum(1 for r in evaluation_results if r['grade'] == 'CORRECT')\n",
    "total_count = len(evaluation_results)\n",
    "\n",
    "print(f\"\\nAccuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\\n\")\n",
    "\n",
    "for i, result in enumerate(evaluation_results, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"\\nGround Truth: {result['ground_truth']}\")\n",
    "    print(f\"\\nPrediction: {result['prediction']}\")\n",
    "    print(f\"\\nGrade: {result['grade']}\")\n",
    "    print(f\"\\nEvaluation: {result['evaluation']}\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. **Document Loading**: Fetched current events from Wikipedia\n",
    "2. **Embeddings**: Created vector representations using Amazon Titan Embeddings\n",
    "3. **Vector Store**: Built an in-memory vector store for similarity search\n",
    "4. **Q&A Chain**: Implemented a retrieval-based question answering system\n",
    "5. **Evaluation**: Generated test cases and evaluated the system using Nova Lite\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Embeddings** capture semantic meaning of text\n",
    "- **Vector similarity search** retrieves relevant documents\n",
    "- **RAG (Retrieval Augmented Generation)** combines retrieval with generation\n",
    "- **LLM-based evaluation** uses language models to assess quality\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different retrieval parameters (k value)\n",
    "- Try different prompting strategies\n",
    "- Add more sophisticated chunking strategies\n",
    "- Implement other chain types (map-reduce, refine, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
