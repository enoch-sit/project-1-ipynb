{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFTMiM0TN_1Y"
      },
      "source": [
        "# Teaching LangChain Function Calling for Beginners\n",
        "\n",
        "Welcome to this beginner-friendly Colab notebook! We'll learn about **LangChain Function Calling** (also known as Tool Calling). This feature lets Large Language Models (LLMs) decide when to call external \"tools\" (like APIs) to get real-world data, instead of just generating text from their training.\n",
        "\n",
        "By the end, you'll build a simple app that asks an LLM for the current weather in Hong Kong using a real weather API.\n",
        "\n",
        "**What you'll need:**\n",
        "- A free Google Colab account.\n",
        "- An API key for the Azure OpenAI endpoint (provided in the docs: `https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions`). If you don't have one, ask your instructor or use a placeholder.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR0lk5cWN_1Z"
      },
      "source": [
        "## Step 1: Install Required Packages\n",
        "\n",
        "Run this cell to install LangChain, the OpenAI integration, and Requests (for API calls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zRUqQ05PN_1Z",
        "outputId": "58cf537d-b174-4e8f-9236-d3cb5013c9cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_openai requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn28xteIN_1Z"
      },
      "source": [
        "## Step 2: Import Libraries and Set Up Your API Key\n",
        "\n",
        "We'll import the necessary modules and set up the Azure OpenAI model. Replace `'your-api-key-here'` with your actual API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7e2J5sHN_1Z",
        "outputId": "4ab1debd-be65-4a5d-fc3e-02c977e05f9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base URL: https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=/openai/deployments/gpt-4o-mini/\n",
            "API Version: 2024-02-15-preview\n",
            "Deployment: gpt-4o-mini\n",
            "f0Ml4DrKVyeXt19VrdGmmzrqJffWfABGLxTiaeaNkeAZxVX3prznwKxpDoS2H6UXzGLPVdxO\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Set your Azure OpenAI API key (keep it secret! In Colab, you can use os.environ for security)\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('eduhkkey')\n",
        "\n",
        "# Set up the Azure OpenAI model (using gpt-4o-mini as per docs)\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",  # Use a recent version\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0,  # Low temperature for consistent tool calling\n",
        "    streaming=False,  # Non-streaming for simplicity\n",
        ")\n",
        "\n",
        "# The actual endpoint used internally\n",
        "print(f\"Base URL: {llm.client._client._base_url}\")\n",
        "print(f\"API Version: {llm.openai_api_version}\")\n",
        "print(f\"Deployment: {llm.deployment_name}\")\n",
        "print(os.environ[\"AZURE_OPENAI_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dnXc4BAN_1Z"
      },
      "source": [
        "**Quick Explanation:**\n",
        "- `AzureChatOpenAI` connects to the Azure endpoint you provided.\n",
        "- `temperature=0` makes the model more deterministic (good for beginners).\n",
        "- We're using non-streaming mode to get full responses at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roheiVrGN_1a"
      },
      "source": [
        "## Step 3: Understand Function Calling\n",
        "\n",
        "**What is Function Calling?**\n",
        "- LLMs like GPT can't access the internet or real-time data directly.\n",
        "- Function Calling lets the LLM \"call\" a predefined function (tool) with parameters.\n",
        "- Example: User asks \"What's the weather in Hong Kong?\" → LLM calls a `get_current_weather` tool → Tool fetches data from an API → LLM summarizes it.\n",
        "\n",
        "In LangChain:\n",
        "- Define tools with the `@tool` decorator (includes name, description, and parameters).\n",
        "- \"Bind\" the tool to the LLM so it knows when/how to use it.\n",
        "- Invoke the LLM with a user query, and it handles the rest!\n",
        "\n",
        "Our tool will use the **Hong Kong Observatory (HKO) Open Data Weather API** to get real-time weather (e.g., temperature, humidity)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JceDN-BN_1a"
      },
      "source": [
        "## Step 4: Define the \"Get Current Weather\" Tool\n",
        "\n",
        "We'll create a tool that calls the HKO API for current weather reports (`dataType=rhrread`). It's Hong Kong-specific, so no location parameter needed for simplicity.\n",
        "\n",
        "The tool fetches JSON data and returns a formatted string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bL2DJ_qTN_1a"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def get_current_weather(place: str) -> str:\n",
        "    \"\"\"\n",
        "    Get the current weather in a given place (e.g., 'Hong Kong').\n",
        "\n",
        "    This tool uses the Hong Kong Observatory API to fetch real-time data like temperature and humidity.\n",
        "    For now, it only works for places in Hong Kong.\n",
        "\n",
        "    Args:\n",
        "        place (str): The place name (e.g., \"Hong Kong\").\n",
        "\n",
        "    Returns:\n",
        "        str: A summary of the current weather.\n",
        "    \"\"\"\n",
        "    if \"hong kong\" not in place.lower():\n",
        "        return \"Sorry, this tool only supports weather for Hong Kong locations.\"\n",
        "\n",
        "    # HKO API endpoint for current weather report\n",
        "    url = \"https://data.weather.gov.hk/weatherAPI/opendata/weather.php\"\n",
        "    params = {\n",
        "        \"dataType\": \"rhrread\",\n",
        "        \"lang\": \"en\"  # English\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params, timeout=10)\n",
        "        response.raise_for_status()  # Raise error for bad status\n",
        "        data = response.json()\n",
        "\n",
        "        # Extract key info (corrected paths based on actual API structure)\n",
        "        update_time = data.get(\"updateTime\", \"Unknown\")\n",
        "        temperature_data = data.get(\"temperature\", {}).get(\"data\", [])\n",
        "        humidity_data = data.get(\"humidity\", {}).get(\"data\", [])\n",
        "        warnings = data.get(\"warningMessage\", []) or data.get(\"specialWxTips\", [])\n",
        "\n",
        "        # Get first station's temp and humidity (e.g., King's Park or HKO)\n",
        "        temp = temperature_data[0][\"value\"] if temperature_data else \"N/A\"\n",
        "        hum = humidity_data[0][\"value\"] if humidity_data else \"N/A\"\n",
        "\n",
        "        summary = f\"Current weather in Hong Kong (reported at {update_time}):\\n\"\n",
        "        summary += f\"- Temperature: {temp}°C\\n\"\n",
        "        summary += f\"- Humidity: {hum}%\\n\"\n",
        "\n",
        "        if warnings:\n",
        "            summary += f\"- Warnings/Tips: {'; '.join(warnings)}\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching weather: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFeQ6cddN_1a"
      },
      "source": [
        "**Quick Explanation:**\n",
        "- `@tool` turns the function into a LangChain tool. The docstring describes it for the LLM.\n",
        "- We use `requests.get` to call the HKO API (base URL from docs).\n",
        "- Parse the JSON response (structure from HKO docs: nested `data` with `temperature`, `humidity`, etc.).\n",
        "- Return a human-readable string. In a real app, you'd handle more fields!\n",
        "\n",
        "Test the tool standalone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdoMUBYsN_1a",
        "outputId": "b8e806f3-9f00-4e11-ca3e-73a0472f092d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current weather in Hong Kong (reported at 2025-09-24T20:21:00+08:00):\n",
            "- Temperature: 26°C\n",
            "- Humidity: 93%\n",
            "- Warnings/Tips: The Tropical Cyclone Signal No. 3 was issued at 8:20 p.m.; The Landslip Warning has been issued.; The Special Announcement of Flooding in Northern New Territories has been issued.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick test of the tool\n",
        "print(get_current_weather.invoke({\"place\": \"Hong Kong\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHFHIaUhN_1a"
      },
      "source": [
        "## Step 5: Bind the Tool to the LLM\n",
        "\n",
        "Now, \"bind\" the tool to our LLM. This tells the model it can use `get_current_weather` when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20HPlDiKN_1a",
        "outputId": "e539a3e5-c2d3-4a52-e27a-7f4aea97cd3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.utils.pydantic.get_current_weather'>\n"
          ]
        }
      ],
      "source": [
        "# Bind the tool to the LLM\n",
        "llm_with_tools = llm.bind_tools([get_current_weather])\n",
        "\n",
        "# Print the tool schema directly (what the LLM sees)\n",
        "print(get_current_weather.get_input_schema())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Uvz0i64N_1a"
      },
      "source": [
        "**What happens under the hood?**\n",
        "- The LLM gets the tool's schema (name, description, parameters).\n",
        "- When you query, the LLM decides: \"Do I need this tool?\" If yes, it outputs a \"tool call\" with args.\n",
        "- LangChain executes the tool and feeds the result back to the LLM for a final response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEU5fJd0N_1a"
      },
      "source": [
        "## Step 6: Create a Simple Chain and Run Examples\n",
        "\n",
        "We'll use `create_tool_calling_agent` for a basic agent that handles tool calls automatically. (this is easier than manual parsing.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4waYj_M9XbET",
        "outputId": "d3dbabc2-563b-41fa-abf6-fbf304d1a387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw JSON Output: {\n",
            "  \"use_tool\": true,\n",
            "  \"tool_name\": \"get_current_weather\",\n",
            "  \"args\": {\"place\": \"Hong Kong\"}\n",
            "}\n",
            "Tool Executed! Result: Current weather in Hong Kong (reported at 2025-09-24T20:21:00+08:00):\n",
            "- Temperature: 26°C\n",
            "- Humidity: 93%\n",
            "- Warnings/Tips: The Tropical Cyclone Signal No. 3 was issued at 8:20 p.m.; The Landslip Warning has been issued.; The Special Announcement of Flooding in Northern New Territories has been issued.\n",
            "\n",
            "Final Summary: Hello! Right now, it's a warm 26°C in Hong Kong with quite high humidity at 93%. Please be aware that there are several warnings in effect: a Tropical Cyclone Signal No. 3 was issued, a Landslip Warning is active, and there's also a Special Announcement regarding flooding in the Northern New Territories. Stay safe and take care!\n"
          ]
        }
      ],
      "source": [
        "# Fixed Step-by-Step Forcing: JSON-Only Tool Decision + Clean Summary\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import json\n",
        "\n",
        "def force_tool_via_json(user_input: str):\n",
        "    # Phase 1: Prompt for JSON-only tool decision (strict format)\n",
        "    json_prompt = SystemMessage(content=\"\"\"You are a tool-calling assistant. Analyze the user query and respond\n",
        "    **ONLY** with valid JSON in this exact format:\n",
        "    {\n",
        "      \"use_tool\": true/false,  // true if query is about CURRENT weather in Hong Kong\n",
        "      \"tool_name\": \"get_current_weather\" or null,\n",
        "      \"args\": {\"place\": \"string\"} or null  // e.g., {\"place\": \"Hong Kong\"}\n",
        "    }\n",
        "    Do NOT output any other text, explanations, or markdown. If not current weather, set use_tool=false.\"\"\")\n",
        "\n",
        "    messages = [json_prompt, HumanMessage(content=user_input)]\n",
        "\n",
        "    try:\n",
        "        json_response = llm_with_tools.invoke(messages)  # Use bound LLM for schema awareness\n",
        "        json_str = json_response.content.strip()  # Extract content\n",
        "        print(\"Raw JSON Output:\", json_str)  # Debug: See what it outputs\n",
        "\n",
        "        # Phase 2: Parse and execute\n",
        "        tool_plan = json.loads(json_str)\n",
        "        if tool_plan.get(\"use_tool\") and tool_plan.get(\"tool_name\") == \"get_current_weather\":\n",
        "            args = tool_plan.get(\"args\", {})\n",
        "            tool_result = get_current_weather.invoke(args)\n",
        "            print(\"Tool Executed! Result:\", tool_result)\n",
        "\n",
        "            # Phase 3: NEW fresh messages for summarization (no JSON prompt!)\n",
        "            summary_system = SystemMessage(content=\"\"\"You are a helpful weather assistant.\n",
        "            Summarize the provided tool result in a natural, friendly response to the user's query.\n",
        "            Include key details like temperature, humidity, and warnings. Keep it concise and engaging.\"\"\")\n",
        "\n",
        "            # Fresh chain: User query + tool result only\n",
        "            summary_messages = [\n",
        "                summary_system,\n",
        "                HumanMessage(content=user_input),\n",
        "                HumanMessage(content=f\"Tool result: {tool_result}\")\n",
        "            ]\n",
        "            final_response = llm.invoke(summary_messages)  # Use unbound LLM for free-form text\n",
        "            return final_response.content\n",
        "        else:\n",
        "            return \"No tool needed for this query.\"\n",
        "    except json.JSONDecodeError:\n",
        "        return \"Error: LLM didn't output valid JSON. Try rephrasing.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Test it\n",
        "user_query = \"What's the current weather in Hong Kong?\"\n",
        "result = force_tool_via_json(user_query)\n",
        "print(\"Final Summary:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sltOecJSN_1b"
      },
      "source": [
        "## Step 7: What's Next?\n",
        "\n",
        "Congrats! You've built your first LangChain Function Calling app. How to apply this to your project?\n",
        "\n",
        "**Resources:**\n",
        "- [LangChain Docs: Tools](https://python.langchain.com/docs/modules/agents/tools/)\n",
        "- [HKO API Full Docs](https://www.hko.gov.hk/en/weatherAPI/doc/files/HKO_Open_Data_API_Documentation.pdf)\n",
        "- Questions? Ask in the comments!\n",
        "\n",
        "Happy coding! 🌤️"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}